{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guimalcantara/-Estrategia_Trading_NLP_e_Analise_De_Sentimento/blob/main/Estrategia_Trading_NLP_e_Analise_De_Sentimento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "83708667-4fdc-1563-7b3a-06b6575d2865",
        "id": "QBBpU6_AaVHH"
      },
      "source": [
        "# NLP e Estratégia de Negociação Baseada em Análise de Sentimento\n",
        "\n",
        "\n",
        "> Adicionar aspas\n",
        "\n",
        "\n",
        "\n",
        "Neste estudo de caso, usamos NLP para construir uma estratégia de negociação combinando alguns dos conceitos que abordamos em alguns dos capítulos anteriores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQo-4RGoaVHL"
      },
      "source": [
        "\n",
        "<a id='1'></a>\n",
        "# 2. Introdução - Carregando os dados e pacotes python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M-1jYTcaVHL"
      },
      "source": [
        "<a id='2.1'></a>\n",
        "## 2.1. Carregando os pacotes python\n",
        "\n",
        "Como primeiro passo, verificamos se os pacotes adicionais necessários estão presentes, caso contrário, instalamos. Eles são verificados separadamente, pois não estão incluídos em requirement.txt, pois não são usados para todos os estudos de caso."
      ]
    },
    {
      "source": [
        "import pkg_resources\n",
        "import pip\n",
        "installedPackages = {pkg.key for pkg in pkg_resources.working_set}\n",
        "required = {'nltk', 'spacy', 'textblob', 'backtrader'}\n",
        "missing = required - installedPackages\n",
        "if missing:\n",
        "    !pip install --upgrade pip #The line should be indented\n",
        "    !pip install nltk==3.9.1  #The line should be indented\n",
        "    !pip install textblob==0.17.1 #The line should be indented\n",
        "    !pip install spacy==3.7.2 #The line should be indented\n",
        "    !pip install backtrader==1.9.74.123 #The line should be indented\n",
        "    !pip install tensorflow\n",
        "    !pip install keras\n",
        "    !pip install yfinance --upgrade"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zSKW8FVhtTbd",
        "collapsed": true
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "source": [
        "# Baixa os modelos de linguagem SpaCy necessários para NLP\n",
        "!python -m spacy download pt_core_news_lg\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!python -m spacy download en_core_web_lg\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5K-OPhi2xffh",
        "outputId": "9b194b18-3caf-417e-ac4b-d7eb9adea0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.7.0/pt_core_news_lg-3.7.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from pt-core-news-lg==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2025.4.26)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from pt-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2025.4.26)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.1)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# DOWNLOADS E CONFIGURAÇÃO DO AMBIENTE NLP\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "import nltk.data\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download de recursos do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Dicionário para armazenar os modelos carregados\n",
        "spacy_models = {}\n",
        "\n",
        "# Função segura para carregar modelos do spaCy\n",
        "def carregar_modelo_spacy(nome_modelo):\n",
        "    try:\n",
        "        return spacy.load(nome_modelo)\n",
        "    except OSError:\n",
        "        print(f\"Modelo {nome_modelo} não encontrado. Instale com:\")\n",
        "        print(f\"!python -m spacy download {nome_modelo}\")\n",
        "        return None\n",
        "\n",
        "# Carrega os modelos em português\n",
        "spacy_models[\"pt\"] = carregar_modelo_spacy(\"pt_core_news_lg\") or carregar_modelo_spacy(\"pt_core_news_sm\")\n",
        "\n",
        "# Carrega o modelo em inglês (obrigatório para headlines internacionais)\n",
        "spacy_models[\"en\"] = carregar_modelo_spacy(\"en_core_web_lg\")\n",
        "\n",
        "# Verifica se os modelos foram carregados corretamente\n",
        "assert spacy_models[\"pt\"] is not None, \"Modelo SpaCy para português não carregado.\"\n",
        "assert spacy_models[\"en\"] is not None, \"Modelo SpaCy para inglês não carregado.\"\n",
        "\n",
        "# ========================================\n",
        "# TESTES DE FUNCIONAMENTO DOS MODELOS\n",
        "# ========================================\n",
        "\n",
        "# Testa com uma frase em português\n",
        "doc_pt = spacy_models[\"pt\"](\"Esta é uma frase de exemplo.\")\n",
        "print(\"Modelo PT:\", [(w.text, w.pos_) for w in doc_pt])\n",
        "\n",
        "# Testa com uma frase em inglês\n",
        "doc_en = spacy_models[\"en\"](\"This is a sample sentence.\")\n",
        "print(\"Modelo EN:\", [(w.text, w.pos_) for w in doc_en])\n"
      ],
      "metadata": {
        "id": "xuEAWihGoXBD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIVYXanwaVHM"
      },
      "source": [
        "Vamos carregar as bibliotecas\n",
        "\n",
        "As bibliotecas **`scikit-learn`** (como `MLPClassifier`, `RandomForestClassifier`, `LogisticRegression`, etc.) são compatíveis com qualquer idioma, incluindo o **português brasileiro**. Isso ocorre porque os modelos de aprendizado de máquina da `scikit-learn` não são sensíveis ao idioma, mas sim às características dos dados (números, textos, etc.).\n",
        "\n",
        "### Como funciona a compatibilidade com o português:\n",
        "\n",
        "1. **Modelos de classificação**:\n",
        "\n",
        "   * Os classificadores como **`MLPClassifier`**, **`RandomForestClassifier`**, **`LogisticRegression`**, etc., funcionam com **qualquer tipo de dado**, incluindo textos em português. No caso de textos, o que importa são as **features (características)** extraídas do texto, como a contagem de palavras, a frequência de termos, ou a presença de palavras específicas.\n",
        "   * O modelo não \"entende\" diretamente o idioma, mas trabalha com **representações numéricas dos dados**, como as que você extrai ao usar técnicas como **TF-IDF** ou **Word Embeddings** (como **Word2Vec**, **GloVe**, ou **FastText**).\n",
        "\n",
        "2. **Extração de características do texto**:\n",
        "   Para usar esses classificadores com texto em português, é necessário representar as palavras ou frases em uma forma que o modelo consiga entender. As técnicas comuns incluem:\n",
        "\n",
        "   * **Tokenização**: Dividir o texto em palavras ou unidades menores.\n",
        "   * **Vetorização**: Transformar essas palavras em números, utilizando métodos como:\n",
        "\n",
        "     * **Contagem de palavras** (`CountVectorizer`)\n",
        "     * **TF-IDF** (`TfidfVectorizer`)\n",
        "     * **Word Embeddings** (como **Word2Vec**, **GloVe**, ou **FastText**).\n",
        "\n",
        "3. **Análise de Sentimentos**:\n",
        "\n",
        "   * Quando você usa **análise de sentimentos** ou **classificação de texto** com essas bibliotecas, você geralmente treina o modelo com **exemplos de textos rotulados** (por exemplo, tweets ou resenhas em português). O modelo aprenderá padrões, mas o treinamento depende de como você prepara os dados de entrada.\n",
        "\n",
        "4. **Resultados e Avaliação**:\n",
        "\n",
        "   * As métricas como **`classification_report`**, **`confusion_matrix`**, e **`accuracy_score`** também são independentes do idioma. Elas avaliam a performance do modelo com base nos dados de entrada (no caso, textos em português) e as classes preditivas.\n",
        "   * Essas métricas funcionam para qualquer conjunto de dados, independentemente de estar em inglês, português ou outro idioma."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Conjunto de manchetes sintéticas simulando notícias financeiras\n",
        "texts = [\n",
        "    \"Ibovespa sobe com otimismo sobre corte de juros\",\n",
        "    \"Dólar recua após anúncio do Banco Central\",\n",
        "    \"Petrobras tem lucro recorde no segundo trimestre\",\n",
        "    \"Mercado reage mal à nova política fiscal\",\n",
        "    \"Ações da Vale despencam após queda no minério\"\n",
        "]\n",
        "\n",
        "# Criando um Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convertendo os textos para sequências de números\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "print(\"Sequências:\", sequences)\n",
        "\n",
        "# Visualizando o vocabulário gerado pelo Tokenizer\n",
        "print(\"Vocabulário:\", tokenizer.word_index)\n",
        "\n"
      ],
      "metadata": {
        "id": "HEWjF_sPy4lB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d8fee34-f454-2642-8b06-ed719f0317e1",
        "id": "w4KkB_cOaVHM"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas para Processamento de Linguagem Natural (PLN)\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "import nltk\n",
        "import warnings\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')  # Baixa o léxico necessário para análise de sentimentos\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Carregue o modelo de linguagem\n",
        "import pt_core_news_lg\n",
        "nlp = pt_core_news_lg.load()\n",
        "import en_core_web_lg\n",
        "nlp_en = en_core_web_lg.load()\n",
        "# Bibliotecas para processar manchetes de notícias\n",
        "from lxml import etree\n",
        "import json\n",
        "from io import StringIO\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pandas.tseries.offsets import BDay  # Offset de negócios (útil para datas)\n",
        "from scipy.stats.mstats import winsorize  # Método de Winsorização (para lidar com valores extremos)\n",
        "from copy import copy  # Criação de cópias superficiais de objetos\n",
        "\n",
        "# Bibliotecas para classificação e modelagem de sentimentos\n",
        "from sklearn.neural_network import MLPClassifier  # Classificador de redes neurais\n",
        "from sklearn.ensemble import RandomForestClassifier  # Classificador de floresta aleatória\n",
        "from sklearn.linear_model import LogisticRegression  # Regressão logística\n",
        "from sklearn.tree import DecisionTreeClassifier  # Classificador de árvore de decisão\n",
        "from sklearn.neighbors import KNeighborsClassifier  # K-vizinhos mais próximos\n",
        "from sklearn.svm import SVC  # Máquinas de vetores de suporte (SVM)\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # Métricas de avaliação\n",
        "\n",
        "# Bibliotecas para modelagem de sentimento com redes neurais (Deep Learning)\n",
        "#from keras.preprocessing.text import Tokenizer  # Tokenização de texto para redes neurais - Commented out keras import\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # Usando tensorflow.keras ao invés de keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Preenchimento de sequências de texto\n",
        "from tensorflow.keras.models import Sequential  # Modelo sequencial de redes neurais\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding  # Camadas de redes neurais, incluindo a camada de embedding\n",
        "from tensorflow.keras.layers import Embedding  # Camada de embedding para palavras\n",
        "\n",
        "# Bibliotecas para análise estatística e visualização\n",
        "import statsmodels.api as sm  # Análise estatística\n",
        "import seaborn as sns  # Visualização de dados\n",
        "import pandas as pd  # Manipulação de dados\n",
        "import numpy as np  # Funções matemáticas e de álgebra linear\n",
        "import datetime  # Manipulação de datas e horas\n",
        "from datetime import date  # Trabalhando com datas\n",
        "import matplotlib.pyplot as plt  # Visualização de gráficos\n",
        "import yfinance as yf  # Para obter dados financeiros de ações\n",
        "\n",
        "# Bibliotecas adicionais\n",
        "import json  # Manipulação de dados em formato JSON\n",
        "import zipfile  # Trabalhar com arquivos ZIP\n",
        "import os.path  # Manipulação de caminhos de arquivos\n",
        "import sys  # Funções do sistema operacional\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns2puq6oaVHN"
      },
      "outputs": [],
      "source": [
        "#Diable the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LheD3U1baVHN"
      },
      "source": [
        "<a id='2.2'></a>\n",
        "## 2.2. Carregando os dados de preços das ações\n",
        "\n",
        "Os dados de preços das ações são carregados nesta etapa do Yahoo Finance. Os dados carregados são salvos em csv para uso posterior."
      ]
    },
    {
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Mapeamento manual de dias da semana em português\n",
        "dias_semana_pt = {\n",
        "    'Monday': 'Segunda-feira',\n",
        "    'Tuesday': 'Terça-feira',\n",
        "    'Wednesday': 'Quarta-feira',\n",
        "    'Thursday': 'Quinta-feira',\n",
        "    'Friday': 'Sexta-feira',\n",
        "    'Saturday': 'Sábado',\n",
        "    'Sunday': 'Domingo'\n",
        "}\n",
        "\n",
        "# Lista de tickers brasileiros\n",
        "tickers = ['VALE3.SA', 'PETR4.SA', 'ITUB4.SA', 'BBAS3.SA']\n",
        "\n",
        "# Período de análise\n",
        "start = '2022-01-01'\n",
        "end = '2024-12-31'\n",
        "\n",
        "# Lista para acumular DataFrames\n",
        "dados_completos = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    ticker_yf = yf.Ticker(ticker)\n",
        "    dados = ticker_yf.history(start=start, end=end)\n",
        "\n",
        "    # Garante que o índice seja convertido em coluna\n",
        "    dados = dados.reset_index()\n",
        "\n",
        "    # Criação das colunas formatadas\n",
        "    dados['Data'] = dados['Date'].dt.date.astype(str)\n",
        "    dados['Hora'] = dados['Date'].dt.time.astype(str)\n",
        "    dados['Dia_da_semana'] = dados['Date'].dt.day_name().map(dias_semana_pt)\n",
        "\n",
        "    dados['Ticker'] = ticker\n",
        "\n",
        "    # Seleciona e reordena colunas\n",
        "    dados_formatado = dados[[\n",
        "        'Data', 'Dia_da_semana',\n",
        "        'Open', 'High', 'Low', 'Close',\n",
        "        'Volume', 'Dividends', 'Stock Splits', 'Ticker'\n",
        "    ]]\n",
        "\n",
        "    dados_completos.append(dados_formatado)\n",
        "\n",
        "# Concatenação final\n",
        "df_final = pd.concat(dados_completos, ignore_index=True)\n",
        "\n",
        "# Criação de diretório\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Caminhos dos arquivos\n",
        "caminho_csv = \"data/ReturnData.csv\"\n",
        "caminho_excel = \"data/ReturnData.xlsx\"\n",
        "\n",
        "# Salvamento dos arquivos\n",
        "df_final.to_csv(caminho_csv, sep='|', index=False)\n",
        "df_final.to_excel(caminho_excel, index=False, engine='openpyxl')\n",
        "\n",
        "# Exibição interativa (Google Colab)\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(df_final)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GIs0joSWunB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TekJgQDLaVHO"
      },
      "source": [
        "Os dados contêm os tickers e seus retornos. Na próxima etapa, limpamos os dados e garantimos que o ponto de partida seja 2022 e que os NAs nos dados sejam removidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKm_PIPiaVHO"
      },
      "source": [
        "<a id='3'></a>\n",
        "\n",
        "# 3. Preparação dos Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QHqoxxpaVHO"
      },
      "source": [
        "Dividimos a preparação dos dados em algumas etapas da seguinte forma:\n",
        "* Carregando e pré-processando os dados das notícias\n",
        "* Preparando os dados combinados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWTY3xmoaVHO"
      },
      "source": [
        "<a id='3.1'></a>\n",
        "\n",
        "## 3.1  Carregando e pré-processando os dados das notícias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4MMQXksaVHO"
      },
      "source": [
        "Os dados das notícias são baixados do feed de notícias RSS e o arquivo é baixado no formato json e os arquivos json para diferentes datas são mantidos em uma pasta compactada."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.1  Coleta Automatizada de Notícias Financeiras via RSS\n",
        "## Coleta Automatizada de Notícias Financeiras via RSS\n",
        "\n",
        "O script desenvolvido tem como objetivo central coletar notícias publicadas na internet relacionadas a ações brasileiras listadas na B3, utilizando a API RSS do Google News como canal de extração. Para tanto, emprega uma arquitetura em Python composta por módulos consagrados como `requests`, `pandas`, `BeautifulSoup` e `dateutil`, integrando técnicas de *web scraping*, manipulação de dados e limpeza textual.\n",
        "\n",
        "### 1. Definição do Escopo Temporal e Semântico\n",
        "\n",
        "Inicialmente, define-se um dicionário com os termos de busca associados a cada **ticker** de interesse (como `'VALE3.SA'`), permitindo múltiplas variações nomenclaturais relevantes para mecanismos de busca. O intervalo de coleta abrange um período fixo entre 1º de janeiro de 2022 e 31 de dezembro de 2024.\n",
        "\n",
        "A seguir, o código gera **janelas temporais de 6 dias** consecutivos para modular a busca. Esta divisão busca contornar limitações de resposta dos serviços RSS e assegurar granularidade temporal adequada para análise posterior.\n",
        "\n",
        "### 2. Consulta ao Google News RSS\n",
        "\n",
        "Para cada janela de tempo e para cada ticker, é construída uma URL parametrizada com uma *query* em linguagem booleana — incluindo todos os termos relacionados ao ativo — e delimitadores temporais `after` e `before`. A requisição HTTP é feita com cabeçalho que simula um navegador, e os resultados são processados com o **BeautifulSoup**, que permite extrair título, descrição, data, horário e URL de cada notícia.\n",
        "\n",
        "Além disso, a biblioteca `langdetect` é utilizada para inferir o idioma da manchete, o que é uma prática comum para filtrar ruídos em bases multilinguísticas. Todo esse processo é encapsulado em uma função robusta com tratamento de exceções para garantir continuidade da execução mesmo em casos de falha de rede ou estrutura HTML malformada.\n",
        "\n",
        "### 3. Estruturação dos Dados\n",
        "\n",
        "As notícias coletadas são armazenadas como dicionários em uma lista principal. Ao final, esse material é convertido em um `DataFrame` do pandas para facilitar operações de tratamento e análise.\n",
        "\n",
        "#### Deduplicação e Limpeza\n",
        "\n",
        "A deduplicação se dá com base em manchetes normalizadas (`headline_clean`), onde caracteres não alfanuméricos são removidos e o texto é transformado em minúsculas. A contagem de repetições é calculada por data e título, permitindo a ordenação por relevância.\n",
        "\n",
        "### 4. Exportação dos Dados\n",
        "\n",
        "Os dados únicos, organizados e limpos, são exportados em formato JSON, com codificação UTF-8 preservada (`force_ascii=False`) e estrutura orientada a registros. O arquivo final é salvo localmente na pasta `dados`.\n",
        "\n",
        "### 5. Feedback ao Usuário\n",
        "\n",
        "A aplicação fornece um resumo estatístico ao final da execução, indicando o número total de notícias únicas coletadas, o caminho do arquivo salvo e a distribuição de notícias por ticker.\n",
        "\n",
        "---\n",
        "\n",
        "## Considerações Técnicas\n",
        "\n",
        "A abordagem é eficiente, modular e robusta. Utiliza técnicas idiomáticas de Python e boas práticas como:\n",
        "\n",
        "* Modularização por funções;\n",
        "* Uso de `try/except` para tolerância a falhas;\n",
        "* Normalização textual para comparação semântica;\n",
        "* Aplicação de `tqdm` para *feedback visual* com barra de progresso;\n",
        "* Estrutura de dados bem organizada para exportação e uso posterior em análises de NLP ou modelagem financeira.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ctwhbp3Ngkqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coleta de notícias"
      ],
      "metadata": {
        "id": "LkXxQtY3N79f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q beautifulsoup4 pandas python-dateutil langdetect tqdm"
      ],
      "metadata": {
        "id": "Oe_-KGX1cWOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# IMPORTAÇÃO DE BIBLIOTECAS NECESSÁRIAS\n",
        "# ===============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# ===============================================================\n",
        "# DEFINIÇÃO DOS TICKERS E TERMOS DE BUSCA ASSOCIADOS\n",
        "# ===============================================================\n",
        "\n",
        "termos_de_busca = {\n",
        "    'VALE3.SA': ['VALE3', 'VALE3.SA', 'BVMF:VALE3'],\n",
        "    'PETR4.SA': ['PETR4', 'PETR4.SA', 'BVMF:PETR4'],\n",
        "    'ITUB4.SA': ['ITUB4', 'ITUB4.SA', 'BVMF:ITUB4'],\n",
        "    'BBAS3.SA': ['BBAS3', 'BBAS3.SA', 'BVMF:BBAS3']\n",
        "}\n",
        "\n",
        "# Intervalo de datas para busca de notícias\n",
        "inicio_coleta = datetime.strptime('2022-01-01', '%Y-%m-%d')\n",
        "fim_coleta    = datetime.strptime('2024-12-31', '%Y-%m-%d')\n",
        "\n",
        "# ===============================================================\n",
        "# FUNÇÃO PARA GERAR INTERVALOS MENSAIS ENTRE DUAS DATAS\n",
        "# ===============================================================\n",
        "\n",
        "def gerar_periodos_mensais(start, end):\n",
        "    periodos = []\n",
        "    atual = start\n",
        "    while atual < end:\n",
        "        fim_mes = atual + relativedelta(months=1) - relativedelta(days=1)\n",
        "        if fim_mes > end:\n",
        "            fim_mes = end\n",
        "        periodos.append((atual, fim_mes))\n",
        "        atual = fim_mes + relativedelta(days=1)\n",
        "    return periodos\n",
        "\n",
        "# ===============================================================\n",
        "# FUNÇÃO PARA COLETAR NOTÍCIAS VIA GOOGLE NEWS RSS\n",
        "# ===============================================================\n",
        "\n",
        "def google_news_rss_range(ticker, termos, data_inicio, data_fim):\n",
        "    # Monta query de busca com OR entre termos\n",
        "    query = \"(\" + \" OR \".join([f'\"{t}\"' if ' ' in t else t for t in termos]) + \")\"\n",
        "\n",
        "    # Monta a URL do RSS com filtros de data\n",
        "    url = (\n",
        "        'https://news.google.com/rss/search'\n",
        "        f'?q={query}+after:{data_inicio.strftime(\"%Y-%m-%d\")}+before:{data_fim.strftime(\"%Y-%m-%d\")}'\n",
        "        '&hl=pt-BR&gl=BR&ceid=BR:pt-419'\n",
        "    )\n",
        "\n",
        "    noticias = []\n",
        "\n",
        "    try:\n",
        "        # Requisição HTTP com cabeçalho do navegador\n",
        "        resposta = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
        "        soup = BeautifulSoup(resposta.content, 'xml')\n",
        "\n",
        "        # Percorre cada item do RSS\n",
        "        for item in soup.find_all('item'):\n",
        "            pub_date = datetime.strptime(item.pubDate.text, '%a, %d %b %Y %H:%M:%S %Z')\n",
        "\n",
        "            # Filtra por data\n",
        "            if not (data_inicio <= pub_date <= data_fim):\n",
        "                continue\n",
        "\n",
        "            titulo  = item.title.text\n",
        "            link    = item.link.text\n",
        "            resumo  = BeautifulSoup(item.description.text, 'html.parser').get_text()\n",
        "\n",
        "            # Valida se algum termo está presente no título ou link\n",
        "            if any(term.lower() in (titulo + link).lower() for term in termos):\n",
        "                noticias.append({\n",
        "                    'ticker': ticker,\n",
        "                    'date': pub_date.strftime('%Y-%m-%d'),\n",
        "                    'time': pub_date.strftime('%H:%M'),\n",
        "                    'headline': titulo,\n",
        "                    'summary': resumo,\n",
        "                    'source': 'Google News RSS',\n",
        "                    'url': link,\n",
        "                    'language': 'pt',\n",
        "                    'scraped_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                })\n",
        "\n",
        "        return noticias\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERRO] Falha ao buscar RSS para {ticker} ({data_inicio} a {data_fim}): {e}\")\n",
        "        return []\n",
        "\n",
        "# ===============================================================\n",
        "# EXECUÇÃO DA COLETA E ARMAZENAMENTO EM ARQUIVOS JSON\n",
        "# ===============================================================\n",
        "\n",
        "# Criação do diretório de saída\n",
        "os.makedirs(\"noticias_json\", exist_ok=True)\n",
        "\n",
        "# Geração dos períodos mensais\n",
        "periodos = gerar_periodos_mensais(inicio_coleta, fim_coleta)\n",
        "\n",
        "# Laço principal para cada ticker\n",
        "for ticker, termos in termos_de_busca.items():\n",
        "    todas_noticias = []\n",
        "\n",
        "    for data_ini, data_fim in periodos:\n",
        "        lote = google_news_rss_range(ticker, termos, data_ini, data_fim)\n",
        "        todas_noticias.extend(lote)\n",
        "\n",
        "    # Salvamento em arquivo JSON por ticker\n",
        "    if todas_noticias:\n",
        "        caminho_arquivo = os.path.join(\"noticias_json\", f\"{ticker}.json\")\n",
        "        with open(caminho_arquivo, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(todas_noticias, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"[OK] Notícias salvas para {ticker} ({len(todas_noticias)} registros)\")\n",
        "\n",
        "# ===============================================================\n",
        "# COMPACTAÇÃO DOS ARQUIVOS EM UM ARQUIVO ZIP\n",
        "# ===============================================================\n",
        "\n",
        "with zipfile.ZipFile(\"noticias_tickers.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for nome_arquivo in os.listdir(\"noticias_json\"):\n",
        "        caminho = os.path.join(\"noticias_json\", nome_arquivo)\n",
        "        zipf.write(caminho, arcname=nome_arquivo)\n",
        "\n",
        "print(\"\\n✅ Arquivo ZIP gerado com sucesso: noticias_tickers.zip\")\n"
      ],
      "metadata": {
        "id": "Wy35ZZhg1Jv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# ===============================================================\n",
        "# 1. EXTRAÇÃO DOS ARQUIVOS DO ZIP (GOOGLE COLAB: /content)\n",
        "# ===============================================================\n",
        "\n",
        "# Caminho do ZIP no ambiente Colab\n",
        "caminho_zip = \"/content/noticias_tickers.zip\"\n",
        "pasta_extracao = \"/content/noticias_json\"\n",
        "os.makedirs(pasta_extracao, exist_ok=True)\n",
        "\n",
        "# Extração dos arquivos JSON\n",
        "with zipfile.ZipFile(caminho_zip, \"r\") as zipf:\n",
        "    zipf.extractall(pasta_extracao)\n",
        "\n",
        "print(\" Arquivos extraídos para pasta '/content/noticias_json/'\")\n",
        "\n",
        "# ===============================================================\n",
        "# 2. CONSOLIDAÇÃO, DEDUPLICAÇÃO E ANOTAÇÃO DE REPETIÇÕES\n",
        "# ===============================================================\n",
        "\n",
        "# Criação do diretório de saída\n",
        "pasta_saida = \"/content/dados\"\n",
        "os.makedirs(pasta_saida, exist_ok=True)\n",
        "\n",
        "# Carregar todos os arquivos JSON extraídos\n",
        "todas_noticias = []\n",
        "for nome_arquivo in os.listdir(pasta_extracao):\n",
        "    if nome_arquivo.endswith(\".json\"):\n",
        "        caminho = os.path.join(pasta_extracao, nome_arquivo)\n",
        "        with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
        "            noticias = json.load(f)\n",
        "            todas_noticias.extend(noticias)\n",
        "\n",
        "# Conversão para DataFrame\n",
        "df = pd.DataFrame(todas_noticias)\n",
        "\n",
        "# Remoção de duplicatas com base em headline e summary\n",
        "df_deduplicado = df.drop_duplicates(subset=['headline', 'summary'])\n",
        "\n",
        "# Contagem de repetições\n",
        "reps = df.groupby(['headline', 'summary']).size().reset_index(name='repetitions')\n",
        "\n",
        "# Junção com coluna repetitions\n",
        "df_final = pd.merge(df_deduplicado, reps, on=['headline', 'summary'], how='left')\n",
        "\n",
        "# Reorganização de colunas (opcional)\n",
        "colunas = ['date', 'time', 'ticker', 'headline', 'summary', 'repetitions',\n",
        "           'source', 'url', 'language', 'scraped_at']\n",
        "df_final = df_final[colunas]\n",
        "\n",
        "# Salvamento do JSON final consolidado\n",
        "saida = os.path.join(pasta_saida, \"noticias_ativos_brasileiros.json\")\n",
        "df_final.to_json(saida, orient='records', indent=2, force_ascii=False)\n",
        "\n",
        "# Resumo\n",
        "print(f\"\\n Arquivo consolidado salvo: {saida}\")\n",
        "print(f\" Total de notícias únicas: {len(df_final)}\")\n"
      ],
      "metadata": {
        "id": "1SmmbtXa25In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLsw66LVaVHO"
      },
      "source": [
        "### Vamos ver o conteúdo do arquivo json\n",
        "Nesta etapa, realiza-se a leitura de um arquivo no formato JSON estruturado contendo notícias financeiras previamente coletadas via Google News RSS. O objetivo é carregar o conteúdo para análise exploratória e manipulação dentro do ambiente Python, utilizando a biblioteca `pandas` ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Carregando arquivo .json consolidado ---\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Caminho do arquivo gerado na etapa anterior\n",
        "caminho_json = \"/content/dados/noticias_ativos_brasileiros.json\"\n",
        "\n",
        "# Carregamento e Leitura\n",
        "with open(caminho_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    noticias_raw = json.load(f)\n",
        "# Mostra o primeiro dicionário (notícia)\n",
        "df_news = pd.read_json(caminho_json, convert_dates=[\"date\", \"scraped_at\"])\n",
        "\n",
        "# Pré-visualização da estrutura bruta\n",
        "noticias_raw[0]\n",
        "# Visualização das primeiras entradas\n",
        "df_news.head(3)\n"
      ],
      "metadata": {
        "id": "TEgW-QAY4Sly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversão para `DataFrame` estruturado\n",
        "\n"
      ],
      "metadata": {
        "id": "8GBY1ZSgh73M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte a coluna 'date' para tipo datetime.date (sem hora)\n",
        "df_news['date'] = df_news['date'].dt.date\n",
        "\n",
        "# Padroniza as colunas\n",
        "df_news = df_news[[\n",
        "    'ticker', 'date', 'time', 'headline', 'summary', 'source', 'url', 'language', 'scraped_at'\n",
        "]]\n",
        "\n",
        "# Remove entradas sem headline ou data\n",
        "df_news.dropna(subset=['headline', 'date'], inplace=True)\n",
        "\n",
        "# Verifica resultado\n",
        "print(f\"Número total de notícias válidas: {len(df_news)}\")\n",
        "df_news.sample(3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mtVWoU0Lhr1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# --- Carregando arquivo .json consolidado ---\n",
        "import pandas as pd\n",
        "import json\n",
        "import google.colab.data_table as data_table\n",
        "from datetime import datetime\n",
        "\n",
        "# Caminho do arquivo gerado na etapa anterior\n",
        "caminho_json = \"dados/noticias_ativos_brasileiros.json\"\n",
        "\n",
        "# Carregamento e Leitura\n",
        "with open(caminho_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    noticias_raw = json.load(f)\n",
        "# Mostra o primeiro dicionário (notícia)\n",
        "df_news = pd.read_json(caminho_json, convert_dates=[\"date\", \"scraped_at\"])\n",
        "\n",
        "# Pré-visualização da estrutura bruta\n",
        "noticias_raw[0]\n",
        "# Visualização das primeiras entradas\n",
        "df_news.head(3)\n",
        "\n",
        "# Converte a coluna 'date' para tipo datetime.date (sem hora)\n",
        "df_news['date'] = df_news['date'].dt.date\n",
        "\n",
        "# Padroniza as colunas\n",
        "df_news = df_news[[\n",
        "    'ticker', 'date', 'time', 'headline', 'summary', 'source', 'url', 'language', 'scraped_at'\n",
        "]]\n",
        "\n",
        "# Remove entradas sem headline ou data\n",
        "df_news.dropna(subset=['headline', 'date'], inplace=True)\n",
        "\n",
        "# Verifica resultado\n",
        "print(f\"Número total de notícias válidas: {len(df_news)}\")\n",
        "df_news.sample(3)\n",
        "\n",
        "# Assign df_news to data_df_news\n",
        "data_df_news = df_news  # This line is crucial to define data_df_news\n",
        "\n",
        "# Visualizar data_df_news (manchetes + tickers + datas)\n",
        "print(\" Visualizando data_df_news (notícias):\")\n",
        "print(data_df_news.dtypes)\n",
        "print(data_df_news.head(5))\n",
        "print(f\"\\nTotal de registros: {len(data_df_news)}\")\n",
        "print(data_df_news['ticker'].value_counts())\n",
        "data_table.DataTable(data_df_news)  # Assuming data_table is already imported"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WXS3Nj_Lx_B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0QKkNmPaVHO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='3.2'></a>\n",
        "\n",
        "## 3.2 Preparando os dados combinados"
      ],
      "metadata": {
        "id": "-Rjj6exCpa_K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U---o-bnaVHP"
      },
      "source": [
        "Nesta etapa, extraímos o retorno de evento, que é o retorno que corresponde ao evento. Fazemos isso porque, às vezes, as notícias são reportadas tardiamente e, outras vezes, são reportadas após o fechamento do mercado. Ter uma janela um pouco mais ampla garante que capturemos a essência do evento. O retorno de evento é definido da seguinte forma: $ R_{t-1} + R_t + R_{t+1} $\n",
        "\n",
        "Onde, $ R_{t-1} $, $ R_{t+1} $ são o retorno antes e depois dos dados da notícia e $ R_{t} $  é o retorno no dia da notícia (ou seja, tempo $t$ )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl --quiet"
      ],
      "metadata": {
        "id": "j6Yw-P2Ek76f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Etapa 3.2.1 — Preparação dos Dados Combinados\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Leitura do arquivo Excel\n",
        "df_ticker_return = pd.read_excel(\"/content/data/ReturnData.xlsx\")\n",
        "\n",
        "# --- Cálculo dos retornos a partir dos dados ajustados ---\n",
        "# Calcula o retorno diário com base na coluna 'Close'\n",
        "df_ticker_return['ret_curr'] = df_ticker_return['Close'].pct_change()\n",
        "\n",
        "# Calcula o retorno do evento (soma de Rt-1 + Rt + Rt+1)\n",
        "df_ticker_return['eventRet'] = (\n",
        "    df_ticker_return['ret_curr'] +\n",
        "    df_ticker_return['ret_curr'].shift(-1) +\n",
        "    df_ticker_return['ret_curr'].shift(1)\n",
        ")\n",
        "\n",
        "# Reset do índice (assumido necessário apenas se 'Data' era índice — neste caso, por segurança)\n",
        "df_ticker_return.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Converte a coluna 'Data' para datetime.date (sem hora)\n",
        "df_ticker_return['date'] = pd.to_datetime(df_ticker_return['Data']).apply(lambda x: x.date())\n",
        "\n",
        "# Renomeia a coluna de ticker, se necessário, para garantir compatibilidade com os dados de notícias\n",
        "df_ticker_return.rename(columns={'Ticker': 'ticker'}, inplace=True)\n",
        "\n",
        "# Verificação rápida\n",
        "print(\"\\n Retornos calculados com sucesso:\")\n",
        "print(df_ticker_return[['Data', 'ticker', 'Close', 'ret_curr', 'eventRet', 'date']].head())\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FxweIa-t5dWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmzlS80gaVHP"
      },
      "source": [
        "Agora que temos todos os dados preparados, prepararemos um dataframe combinado que terá as manchetes das notícias mapeadas para a data, Retorno de Evento e ticker da ação. Este dataframe será usado para análise posterior para o modelo de análise de sentimento e para construir a estratégia de negociação (trading)."
      ]
    },
    {
      "source": [
        "# Junção com Dados de Notícias\n",
        "# --- Leitura das notícias salvas ---\n",
        "noticias_json = \"dados/noticias_ativos_brasileiros.json\"\n",
        "data_df_news = pd.read_json(noticias_json)\n",
        "data_df_news['date'] = pd.to_datetime(data_df_news['date']).dt.date\n",
        "data_df_news = data_df_news[['ticker', 'headline', 'date']]  # apenas o necessário\n",
        "\n",
        "# --- Junção com dados de retorno ---\n",
        "combinedDataFrame = pd.merge(\n",
        "    data_df_news,\n",
        "    df_ticker_return,\n",
        "    how='left',\n",
        "    left_on=['ticker', 'date'],\n",
        "    right_on=['ticker', 'date'] # Changed 'Ticker' to 'ticker' in right_on\n",
        ")\n",
        "\n",
        "# --- Seleção final de colunas e limpeza ---\n",
        "data_df = combinedDataFrame[['ticker', 'headline', 'date', 'eventRet', 'Close']]\n",
        "data_df = data_df.dropna().reset_index(drop=True)\n",
        "\n",
        "# --- Visualização inicial ---\n",
        "data_df.head(3)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Pic538E3C06J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos salvar os dados em um arquivo csv para serem usados posteriormente, para que a etapa de processamento de dados possa ser pulada toda vez que formos realizar a análise."
      ],
      "metadata": {
        "id": "Fm26VIA3nOnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exportação Final para CSV\n",
        "\n",
        "# --- Salvamento do dataset final ---\n",
        "import os\n",
        "os.makedirs(\"dados\", exist_ok=True)\n",
        "caminho_saida = \"/content/AnaliseSentimentoNLP/AnaliseSentimentoNLP/Step3_NewsAndReturnData.csv\"\n",
        "data_df.to_csv(caminho_saida, sep='|', index=False)\n",
        "\n",
        "# --- Resumo da operação ---\n",
        "print(f\"\\n Arquivo final salvo em: {caminho_saida}\")\n",
        "print(f\"Total de registros combinados: {len(data_df)}\")\n",
        "print(data_df.groupby('ticker').size())\n"
      ],
      "metadata": {
        "id": "Rjs1K6V-ed_Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiUfqPQYaVHQ"
      },
      "outputs": [],
      "source": [
        "data_df.dropna().to_csv(r'Data\\Step3_NewsAndReturnData.csv', sep='|', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TEP4lOIaVHS"
      },
      "source": [
        "<a id='3.3'></a>\n",
        "## 3.3 Carregando os dados pré-processados\n",
        "#### Comece a partir desta etapa caso você não queira executar as etapas anteriores de pré-processamento."
      ]
    },
    {
      "source": [
        "# --- Etapa 3.3: Recarregando dados diretamente do CSV (caso queira pular a etapa de preparação) ---\n",
        "print(\"\\n Recarregando dados salvos (para uso direto em análise):\")\n",
        "\n",
        "#caminho_saida is the variable where the csv was saved in the previous cell\n",
        "data_df = pd.read_csv(caminho_saida, sep='|') #Change caminho_csv to caminho_saida\n",
        "data_df = data_df.dropna()\n",
        "\n",
        "# --- Verificação básica ---\n",
        "print(\" Estrutura dos dados carregados:\")\n",
        "print(data_df.dtypes)\n",
        "print(data_df.head(3))\n",
        "print(f\"\\n Total de registros carregados: {data_df.shape[0]}\")\n",
        "print(f\" Total de tickers únicos: {data_df['ticker'].nunique()}\")\n",
        "print(f\" Lista de tickers: {list(data_df['ticker'].unique())}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WwHA5G3ozrMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9NKm3g6aVHT"
      },
      "outputs": [],
      "source": [
        "print(data_df.shape, data_df.ticker.unique().shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2d8V4rsaVHT"
      },
      "source": [
        "Nesta etapa, preparamos um dataframe limpo que com:\n",
        "\n",
        "* **2607 registros** (linhas de manchetes combinadas com dados de retorno),\n",
        "* **5 colunas**: `'ticker'`, `'headline'`, `'date'`, `'eventRet'`, `'Close'`,\n",
        "* **4 tickers únicos**, o que confirma que os dados foram combinados para todas as ações: `'VALE3.SA'`, `'PETR4.SA'`, `'ITUB4.SA'`, `'BBAS3.SA'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHtWyQxaVHT"
      },
      "source": [
        "<a id='4'></a>\n",
        "# 4. Avaliar Modelos para Análise de Sentimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY03CSmPaVHT"
      },
      "source": [
        "Nesta seção, abordaremos as três abordagens diferentes a seguir para obter os sentimentos das notícias, que usaremos para construir a estratégia de negociação (trading).\n",
        "\n",
        "* Modelo predefinido - pacote TextBlob\n",
        "* Modelo Ajustado - Algoritmos de Classificação e LSTM\n",
        "* Modelo baseado em léxico financeiro\n",
        "\n",
        "Também exploraremos a diferença entre as diferentes formas de realizar a análise de sentimento. Vamos seguir os passos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2665Yq-LaVHT"
      },
      "source": [
        "<a id='4.1'></a>\n",
        "## 4.1 - Aplicação e Comparação de Modelos de Sentimento em Português"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5ksyRLFaVHT"
      },
      "source": [
        "Nesta etapa, serão aplicados modelos de linguagem natural pré-treinados, com foco em análise de sentimentos em português, sobre os títulos de notícias financeiras contidos na coluna `headline` do DataFrame `data_df`. O objetivo principal consiste na obtenção de **pontuações contínuas de polaridade**, no intervalo $[-1, +1]$, que serão posteriormente confrontadas com os **retornos de evento** (`eventRet`). A finalidade é investigar potenciais correlações entre o conteúdo semântico dos títulos e o comportamento do mercado.\n",
        "\n",
        "### Modelos Utilizados\n",
        "\n",
        "| Modelo | Nome no Hugging Face                           | Tipo       | Idioma    | Saída original                    | Mapeamento para escore contínuo   |\n",
        "| ------ | ---------------------------------------------- | ---------- | --------- | --------------------------------- | --------------------------------- |\n",
        "| A      | `lucas-leme/FinBERT-PT-BR`                     | Financeiro | Português | `POSITIVE`, `NEGATIVE`, `NEUTRAL` | `-1` (NEG), `0` (NEU), `+1` (POS) |\n",
        "| B      | `sptech-ai/sptech.template.ai.model.sentiment` | Genérico   | Português | `POSITIVE`, `NEGATIVE`            | `-1` (NEG), `+1` (POS)            |\n",
        "| C      | `turing-usp/FinBertPTBR`                       | Financeiro | Português | `POSITIVE`, `NEGATIVE`, `NEUTRAL` | `-1` (NEG), `0` (NEU), `+1` (POS) |\n",
        "\n",
        "### Conversão de Saídas Categóricas para Polaridade Contínua\n",
        "\n",
        "Para garantir a compatibilidade com métodos quantitativos e análises estatísticas, as classificações categóricas dos modelos serão transformadas conforme o seguinte esquema de codificação:\n",
        "\n",
        "* `NEGATIVE` → `-1`\n",
        "* `NEUTRAL` → `0`\n",
        "* `POSITIVE` → `+1`\n",
        "\n",
        "Nos casos em que o modelo fornece **distribuições de probabilidade sobre as classes** (a partir dos *logits*), será adotada uma abordagem probabilística para o cálculo do escore contínuo, conforme a equação:\n",
        "\n",
        "$$\n",
        "\\text{Polaridade} = P(\\text{POSITIVE}) \\cdot (+1) + P(\\text{NEUTRAL}) \\cdot 0 + P(\\text{NEGATIVE}) \\cdot (-1)\n",
        "$$\n",
        "\n",
        "Essa formulação permite capturar com maior fidelidade a incerteza do modelo, oferecendo um resultado mais informativo e expressivo, análogo ao funcionamento da biblioteca `TextBlob`.\n",
        "\n",
        "### Considerações Técnicas\n",
        "\n",
        "Os modelos utilizados não fornecem, de forma nativa, uma métrica de polaridade contínua. No entanto, a conversão dos *logits* para probabilidades por meio da função `softmax` permite a emulação confiável desse comportamento. Tal estratégia é altamente recomendada para aplicações em finanças quantitativas, nas quais a granularidade e a robustez dos dados de entrada são fatores determinantes para a qualidade das inferências e decisões baseadas em modelos.\n",
        "\n",
        "###  Rodar a análise pra esses modelos agora é só por curiosidade mesmo, só pra ver se o ajuste funciona."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Dispositivo de execução\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "Wa5_xZO7gnCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token de acesso ao Hugging Face (pré-configurado em variável de ambiente)\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "def calcular_polaridade_continua(\n",
        "    model_id: str,\n",
        "    n_classes: int,\n",
        "    textos: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"Retorna escore contínuo: P_POS*1 + P_NEU*0 + P_NEG*(-1).\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_token)\n",
        "    model = (AutoModelForSequenceClassification\n",
        "             .from_pretrained(model_id, use_auth_token=hf_token)\n",
        "             .to(device))\n",
        "    model.eval()\n",
        "\n",
        "    def pontuar(texto: str) -> float:\n",
        "        inputs = tokenizer(\n",
        "            texto,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
        "        if n_classes == 3:\n",
        "            # ordem: [NEG, NEU, POS]\n",
        "            return float(probs[2]*1 + probs[1]*0 + probs[0]*-1)\n",
        "        elif n_classes == 2:\n",
        "            # ordem: [NEG, POS]\n",
        "            return float(probs[1]*1 + probs[0]*-1)\n",
        "        else:\n",
        "            return np.nan\n",
        "\n",
        "    return textos.progress_apply(pontuar)\n"
      ],
      "metadata": {
        "id": "9I6hh6zygrkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/Data\\Step3_NewsAndReturnData.csv\"\n",
        "if not os.path.exists(data_path):\n",
        "    raise FileNotFoundError(f\"Arquivo não encontrado em {data_path}\")\n",
        "data_df = pd.read_csv(data_path, sep=\"|\")\n"
      ],
      "metadata": {
        "id": "4JHipp1Kgu98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "def aplicar_modelo_sentimento(model_name, task='text-classification'):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    pipe = pipeline(task=task, model=model, tokenizer=tokenizer)\n",
        "    return pipe\n"
      ],
      "metadata": {
        "id": "JfNysDkZ_6eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Lê o token, nome e e-mail diretamente dos secrets\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n"
      ],
      "metadata": {
        "id": "1EhuUIS1KE8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Defina o dicionário de modelos"
      ],
      "metadata": {
        "id": "thb-1iqKHrvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelos = {\n",
        "    \"FinBERT_PT_BR\": {\n",
        "        \"repo\": \"lucas-leme/FinBERT-PT-BR\",\n",
        "        \"n_classes\": 3\n",
        "    },\n",
        "    \"FinBERT_Turing\": {\n",
        "        \"repo\": \"turing-usp/FinBertPTBR\",\n",
        "        \"n_classes\": 3\n",
        "    },\n",
        "    \"Multilingual_Sentiment\": {\n",
        "        \"repo\": \"tabularisai/multilingual-sentiment-analysis\",\n",
        "        \"n_classes\": 3\n",
        "    }\n",
        "}\n",
        "\n",
        "for nome, cfg in modelos.items():\n",
        "    try:\n",
        "        print(f\"\\nAplicando modelo {nome}...\")\n",
        "        coluna = f\"sentiment_{nome}\"\n",
        "        data_df[coluna] = calcular_polaridade_continua(\n",
        "            model_id=cfg[\"repo\"],\n",
        "            n_classes=cfg[\"n_classes\"],\n",
        "            textos=data_df[\"headline\"]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Erro em {nome}: {e}\")\n"
      ],
      "metadata": {
        "id": "uEtFhtPPg1VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cálculo de Correlação com Retorno de Evento\n",
        "\n"
      ],
      "metadata": {
        "id": "3YDzfBsTH1wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "# 1. Cálculo de correlação e p-valor\n",
        "resultados = []\n",
        "for nome_modelo in modelos.keys():\n",
        "    coluna = f\"sentiment_{nome_modelo}\"\n",
        "    if coluna in data_df.columns:\n",
        "        x = data_df[coluna]\n",
        "        y = data_df[\"eventRet\"]\n",
        "        corr, p_valor = stats.pearsonr(x, y)\n",
        "        resultados.append({\n",
        "            \"Modelo\": nome_modelo,\n",
        "            \"Correlação\": corr,\n",
        "            \"p-valor\": p_valor\n",
        "        })\n",
        "        print(f\"{nome_modelo}: correlação = {corr:.4f}, p-valor = {p_valor:.4g}\")\n",
        "\n",
        "# 2. DataFrame de resultados\n",
        "corr_df = pd.DataFrame(resultados).set_index(\"Modelo\")\n",
        "\n",
        "# 3. Gráfico de barras\n",
        "plt.figure(figsize=(6,4))\n",
        "corr_df[\"Correlação\"].plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.axhline(0, color=\"gray\", linewidth=0.8)\n",
        "plt.title(\"Coeficiente de Correlação entre Sentimento e eventRet\")\n",
        "plt.ylabel(\"Pearson r\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Gráficos de dispersão\n",
        "for nome_modelo in modelos.keys():\n",
        "    coluna = f\"sentiment_{nome_modelo}\"\n",
        "    if coluna in data_df.columns:\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.scatter(data_df[coluna], data_df[\"eventRet\"], alpha=0.4, s=10)\n",
        "        plt.title(f\"Dispersão: {nome_modelo}\")\n",
        "        plt.xlabel(\"Pontuação de Sentimento\")\n",
        "        plt.ylabel(\"Retorno (eventRet)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "XMIsYCNMD79s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='4.2'></a>\n",
        "## 4.2 – Análise de Sentimento com Classificadores Supervisionados e LSTM"
      ],
      "metadata": {
        "id": "pVPqEtK60Uke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bd-hGWitiKIW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jI1INM9RiETr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Construção de Pipeline com LSTM para Classificação Binária\n",
        "\n",
        "Com base no conjunto consolidado contendo as variáveis `headline` e `eventRet`, foi construída uma *pipeline* de classificação binária utilizando redes neurais recorrentes do tipo **LSTM (Long Short-Term Memory)**.\n",
        "\n",
        "### Preparação dos Dados\n",
        "\n",
        "* A variável `eventRet` foi transformada em uma variável alvo binária:\n",
        "\n",
        "  * Valores **≥ 0** foram rotulados como **1** (*retorno positivo*);\n",
        "  * Valores **< 0** foram rotulados como **0** (*retorno negativo*).\n",
        "\n",
        "* As manchetes textuais (`headline`) foram convertidas em vetores semânticos utilizando técnicas de *word embeddings*.\n",
        "\n",
        "* Foram aplicadas as seguintes etapas de pré-processamento:\n",
        "\n",
        "  * **Tokenização**: conversão das palavras em índices numéricos;\n",
        "  * **Padding**: preenchimento das sequências para padronização do comprimento.\n",
        "\n",
        "### Arquitetura do Modelo\n",
        "\n",
        "O modelo foi estruturado com as seguintes camadas:\n",
        "\n",
        "* `Embedding`:\n",
        "\n",
        "  * Dimensão vetorial: **128**\n",
        "\n",
        "* `LSTM`:\n",
        "\n",
        "  * Unidades: **64**\n",
        "  * Aplicação de **dropout** para regularização.\n",
        "\n",
        "* Camada densa final:\n",
        "\n",
        "  * Ativação: **sigmoide**\n",
        "  * Finalidade: produção de probabilidade binária (0 ou 1)\n",
        "\n",
        "### Estratégia de Treinamento\n",
        "\n",
        "* O conjunto de dados foi dividido da seguinte forma:\n",
        "\n",
        "  * **70%** para treino;\n",
        "  * **30%** para teste.\n",
        "\n",
        "* Foram utilizadas técnicas de:\n",
        "\n",
        "  * **Validação cruzada**;\n",
        "  * **Early stopping**, a fim de evitar *overfitting*.\n",
        "\n",
        "### Resultados Obtidos\n",
        "\n",
        "* **Acurácia no conjunto de treino**: **96,6%**\n",
        "* **Acurácia no conjunto de teste**: **91,0%**\n",
        "* **Correlação entre as predições e `eventRet`**: **0,446**\n",
        "* **Matriz de confusão**: evidenciou equilíbrio na classificação entre as classes.\n",
        "\n",
        "### Pós-Treinamento\n",
        "\n",
        "Após o treinamento:\n",
        "\n",
        "* O modelo foi aplicado ao **conjunto completo** de dados do intervalo de 01 de janeiro de 2022 a 31 de dezembro de 2024.\n",
        "* As **predições** foram armazenadas para:\n",
        "\n",
        "  * Análise quantitativa posterior;\n",
        "  * Integração com **simulações de estratégias financeiras**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WzbeoQ7R0f-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install keras tensorflow scikit-learn"
      ],
      "metadata": {
        "id": "_4ZoDUcn1Iaz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HpmjR9aci9kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Parte 2.1 — Definição de sentiments_data\n",
        "# logo após ter carregado data_df:\n",
        "sentiments_data = data_df.copy()\n",
        "\n",
        "# Agora a verificação prossegue sem NameError\n",
        "if 'eventRet' not in sentiments_data.columns:\n",
        "    if 'eventRet' in data_df.columns:\n",
        "        data_df['date']        = pd.to_datetime(data_df['date'],        errors='coerce').dt.date\n",
        "        sentiments_data['date']= pd.to_datetime(sentiments_data['date'],errors='coerce').dt.date\n",
        "        sentiments_data = pd.merge(\n",
        "            sentiments_data,\n",
        "            data_df[['date','ticker','eventRet']],\n",
        "            on=['date','ticker'],\n",
        "            how='left'\n",
        "        )\n",
        "    else:\n",
        "        raise KeyError(\"Coluna 'eventRet' não encontrada em sentiments_data nem em data_df\")"
      ],
      "metadata": {
        "id": "pumKiDTj0fth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove registros sem manchete ou retorno\n",
        "df_modelo = sentiments_data.dropna(subset=[\"headline\", \"eventRet\"]).copy()\n",
        "\n",
        "# Cria variável binária: 1 se eventRet ≥ 0, senão 0\n",
        "df_modelo[\"sentiment\"] = df_modelo[\"eventRet\"].apply(lambda x: 1 if x >= 0 else 0)\n",
        "\n",
        "# Exibe balanceamento\n",
        "print(\"Distribuição dos rótulos:\")\n",
        "print(df_modelo[\"sentiment\"].value_counts())\n"
      ],
      "metadata": {
        "id": "joid12mTkO-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de vetorização por idioma\n",
        "def vetorizar_manchete(texto, idioma='pt'):\n",
        "    modelo = spacy_models.get(idioma, spacy_models['pt'])\n",
        "    doc = modelo(texto)\n",
        "    return doc.vector\n",
        "\n",
        "# Gera matriz de embeddings\n",
        "print(\"Gerando embeddings das manchetes... isso pode demorar.\")\n",
        "X = np.vstack(df_modelo[\"headline\"].apply(lambda x: vetorizar_manchete(str(x))).values)\n",
        "\n",
        "# Define variável alvo y\n",
        "y = df_modelo[\"sentiment\"].values\n"
      ],
      "metadata": {
        "id": "S2mAMqZ-0xl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#Parte 5. Tokenização, sequência e padding para LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configura tokenizador\n",
        "vocabulary_size = 20000\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(df_modelo[\"headline\"])\n",
        "\n",
        "# Converte textos em sequências e aplica padding\n",
        "sequences = tokenizer.texts_to_sequences(df_modelo[\"headline\"])\n",
        "X_seq = pad_sequences(sequences, maxlen=50, padding='post', truncating='post')\n",
        "\n",
        "# Divide em treino (70 %) e teste (30 %)\n",
        "X_train_LSTM, X_test_LSTM, y_train_LSTM, y_test_LSTM = train_test_split(\n",
        "    X_seq,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EY0kqUuu1vf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# KERNEL DEVE SER REINICIALIZADO ANTES DA EXECUÇÃO\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 1. VARIÁVEIS DE AMBIENTE (antes de importar TensorFlow)\n",
        "import os\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # força CPU, se necessário\n",
        "\n",
        "# 2. IMPORTS\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 3. LIMPEZA DE SESSÃO\n",
        "K.clear_session()\n",
        "\n",
        "# 4. PRECISÃO MISTA\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# 5. HYPERPARÂMETROS\n",
        "VOCAB_SIZE   = 10000\n",
        "SEQ_LENGTH   = 50\n",
        "EMBED_DIM    = 64\n",
        "RNN_UNITS    = 16\n",
        "BATCH_SIZE   = 64\n",
        "EPOCHS       = 10\n",
        "VAL_SPLIT    = 0.2\n",
        "\n",
        "# 6. CARREGAMENTO E PREPARO DOS DADOS\n",
        "# Substituir por carregamento real de df_modelo com colunas 'headline' e 'eventRet'\n",
        "# df = pd.read_csv('dados.csv', sep='|')\n",
        "# df_modelo = df.dropna(subset=['headline','eventRet'])\n",
        "# df_modelo['sentiment'] = (df_modelo['eventRet'] >= 0).astype(int)\n",
        "\n",
        "# Tokenização e padding\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df_modelo['headline'])\n",
        "sequences = tokenizer.texts_to_sequences(df_modelo['headline'])\n",
        "X = pad_sequences(sequences, maxlen=SEQ_LENGTH, padding='post', truncating='post')\n",
        "y = df_modelo['sentiment'].values\n",
        "\n",
        "# Split treino+val / teste\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Divisão interna treino / validação\n",
        "n_val = int(VAL_SPLIT * X_train_val.shape[0])\n",
        "X_val = X_train_val[:n_val];   y_val = y_train_val[:n_val]\n",
        "X_train = X_train_val[n_val:];  y_train = y_train_val[n_val:]\n",
        "\n",
        "# 7. PIPELINE tf.data COM CACHE EM DISCO\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "      .shuffle(buffer_size=10000)\n",
        "      .cache(\"cache_train.tfdata\")\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "val_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "      .cache(\"cache_val.tfdata\")\n",
        "      .batch(BATCH_SIZE)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# 8. DEFINIÇÃO DO MODELO (GRU)\n",
        "def build_model() -> Sequential:\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(\n",
        "        input_dim=tokenizer.num_words,\n",
        "        output_dim=EMBED_DIM,\n",
        "        input_length=SEQ_LENGTH\n",
        "    ))\n",
        "    model.add(GRU(\n",
        "        units=RNN_UNITS,\n",
        "        dropout=0.2,\n",
        "        recurrent_dropout=0.2\n",
        "    ))\n",
        "    model.add(Dense(1, activation='sigmoid', dtype='float32'))\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 9. CALLBACKS DE TREINO\n",
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "rlrp = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 10. TREINAMENTO\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[es, rlrp],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 11. PLOT DE MÉTRICAS\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'],     'o-', label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], 'o-', label='Val Loss')\n",
        "plt.title('Curva de Loss')\n",
        "plt.xlabel('Época'); plt.ylabel('Binary Crossentropy')\n",
        "plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "plt.savefig('loss_curve.png', dpi=300); plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['accuracy'],     'o-', label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], 'o-', label='Val Acc')\n",
        "plt.title('Curva de Acurácia')\n",
        "plt.xlabel('Época'); plt.ylabel('Acurácia')\n",
        "plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "plt.savefig('accuracy_curve.png', dpi=300); plt.show()\n"
      ],
      "metadata": {
        "id": "jbZ-l9eDs1EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XH9paND657RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB2uJYG_3VcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Download manual do GloVe (100d):**\n",
        "\n",
        "Acesse o link abaixo e baixe o arquivo necessário:\n",
        "\n",
        "🔗 [https://nlp.stanford.edu/data/glove.6B.zip](https://nlp.stanford.edu/data/glove.6B.zip)\n",
        "\n",
        "2. **Etapas:**\n",
        "\n",
        "* Extraia o conteúdo do `.zip` baixado.\n",
        "* Copie o arquivo `glove.6B.100d.txt` para o diretório onde seu script está localizado (ou especifique o caminho completo no `open()`).\n",
        "* O arquivo ocupa cerca de 350 MB.\n"
      ],
      "metadata": {
        "id": "7tOfoxhms0y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. VARIÁVEIS DE AMBIENTE\n",
        "import os\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "\n",
        "# 2. IMPORTS\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Embedding, GRU, Dense, Input, Bidirectional,\n",
        "    SpatialDropout1D, Layer, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import mixed_precision\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 3. LIMPEZA DE SESSÃO\n",
        "K.clear_session()\n",
        "\n",
        "# 4. PRECISÃO MISTA\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# 5. HYPERPARÂMETROS\n",
        "VOCAB_SIZE   = 10000\n",
        "SEQ_LENGTH   = 50\n",
        "EMBED_DIM    = 100  # Para uso com GloVe 100d\n",
        "RNN_UNITS    = 64\n",
        "BATCH_SIZE   = 64\n",
        "EPOCHS       = 10\n",
        "VAL_SPLIT    = 0.2\n",
        "\n",
        "# 6. CARREGAMENTO E PREPARO DOS DADOS\n",
        "# df_modelo = pd.read_csv('dados.csv', sep='|').dropna(subset=['headline', 'eventRet'])\n",
        "# df_modelo['sentiment'] = (df_modelo['eventRet'] >= 0).astype(int)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df_modelo['headline'])\n",
        "sequences = tokenizer.texts_to_sequences(df_modelo['headline'])\n",
        "X = pad_sequences(sequences, maxlen=SEQ_LENGTH, padding='post', truncating='post')\n",
        "y = df_modelo['sentiment'].values\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=VAL_SPLIT, stratify=y_train_val, random_state=42\n",
        ")\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# 7. MATRIZ DE EMBEDDING PRÉ-TREINADA (GloVe)\n",
        "embedding_index = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = vector\n",
        "\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= VOCAB_SIZE:\n",
        "        continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# 8. CAMADA DE ATENÇÃO PERSONALIZADA\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
        "                                 initializer=\"normal\", trainable=True)\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
        "                                 initializer=\"zeros\", trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
        "        a = tf.nn.softmax(e, axis=1)\n",
        "        output = tf.reduce_sum(x * a, axis=1)\n",
        "        return output\n",
        "\n",
        "# 9. DEFINIÇÃO DO MODELO COM BIDIRECTIONAL, EMBEDDING FIXO, ATTENTION\n",
        "def build_model() -> Model:\n",
        "    inputs = Input(shape=(SEQ_LENGTH,))\n",
        "    x = Embedding(\n",
        "        input_dim=VOCAB_SIZE,\n",
        "        output_dim=EMBED_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=SEQ_LENGTH,\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(GRU(RNN_UNITS, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "    x = AttentionLayer()(x)\n",
        "    outputs = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 10. CALLBACKS\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1)\n",
        "\n",
        "# 11. TREINAMENTO\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[es, rlrp], verbose=1)\n",
        "\n",
        "# 12. PLOT DE MÉTRICAS\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'], 'o-', label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], 'o-', label='Val Loss')\n",
        "plt.title('Curva de Loss')\n",
        "plt.xlabel('Época'); plt.ylabel('Binary Crossentropy')\n",
        "plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "plt.savefig('loss_curve.png', dpi=300); plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['accuracy'], 'o-', label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], 'o-', label='Val Acc')\n",
        "plt.title('Curva de Acurácia')\n",
        "plt.xlabel('Época'); plt.ylabel('Acurácia')\n",
        "plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "plt.savefig('accuracy_curve.png', dpi=300); plt.show()\n"
      ],
      "metadata": {
        "id": "SDAsKqIz2wPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Configurações de hardware e sessão\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # força CPU\n",
        "K.clear_session()\n",
        "\n",
        "# 2. Hiperparâmetros\n",
        "vocabulary_size = 10000   # conforme tokenizador\n",
        "input_length    = 50      # comprimento das sequências\n",
        "embed_dim       = 64      # dimensão do embedding\n",
        "lstm_units      = 32      # unidades LSTM\n",
        "batch_size      = 16      # reduzido para economia de memória\n",
        "epochs          = 10\n",
        "\n",
        "# 3. Definição do modelo\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(\n",
        "        input_dim=vocabulary_size,\n",
        "        output_dim=embed_dim,\n",
        "        input_length=input_length\n",
        "    ))\n",
        "    model.add(LSTM(\n",
        "        units=lstm_units,\n",
        "        dropout=0.2,\n",
        "        recurrent_dropout=0.2\n",
        "    ))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model_LSTM = create_model()\n",
        "\n",
        "# 4. Treinamento com early stopping\n",
        "es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "history = model_LSTM.fit(\n",
        "    X_train_LSTM,\n",
        "    y_train_LSTM,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Plot das curvas de treinamento\n",
        "\n",
        "# 5.1 Curva de Loss\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'],     label='Train Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss',   marker='o')\n",
        "plt.title('Curva de Loss durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Binary Crossentropy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_curve.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 5.2 Curva de Acurácia\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['accuracy'],      label='Train Acc', marker='o')\n",
        "plt.plot(history.history['val_accuracy'],  label='Val Acc',   marker='o')\n",
        "plt.title('Curva de Acurácia durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('accuracy_curve.png', dpi=300)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VxYQg4SKqhQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Parte 6. Construção e treinamento do modelo LSTM (ajustado para menor consumo de memória)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "import os\n",
        "\n",
        "# Forçar CPU para evitar OOM na GPU (caso deseje GPU, remova estas linhas)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "K.clear_session()\n",
        "\n",
        "def create_model(input_length: int = 50) -> Sequential:\n",
        "    model = Sequential()\n",
        "    # Embedding com dimensão reduzida\n",
        "    model.add(Embedding(\n",
        "        input_dim=vocabulary_size,   # conforme definido na etapa anterior\n",
        "        output_dim=64,               # reduzido de 128 para 64\n",
        "        input_length=input_length\n",
        "    ))\n",
        "    # LSTM com menos unidades\n",
        "    model.add(LSTM(\n",
        "        units=64,                    # reduzido de 64 para 32\n",
        "        dropout=0.2,\n",
        "        recurrent_dropout=0.2\n",
        "    ))\n",
        "    # Camada de saída\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model_LSTM = create_model(input_length=50)\n",
        "\n",
        "# Treinamento com batch menor\n",
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model_LSTM.fit(\n",
        "    X_train_LSTM,\n",
        "    y_train_LSTM,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Plot das curvas de treinamento\n",
        "\n",
        "# 5.1 Curva de Loss\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'],     label='Train Loss', marker='o')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss',   marker='o')\n",
        "plt.title('Curva de Loss durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Binary Crossentropy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('loss_curve.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# 5.2 Curva de Acurácia\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['accuracy'],      label='Train Acc', marker='o')\n",
        "plt.plot(history.history['val_accuracy'],  label='Val Acc',   marker='o')\n",
        "plt.title('Curva de Acurácia durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('accuracy_curve.png', dpi=300)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ir4xljn0lZgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    auc as sk_auc\n",
        ")\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# --- Etapa 7: Avaliação do modelo LSTM ---\n",
        "\n",
        "# 1. Geração das predições binárias\n",
        "y_train_prob = model_LSTM.predict(X_train_LSTM).flatten()\n",
        "y_test_prob  = model_LSTM.predict(X_test_LSTM).flatten()\n",
        "\n",
        "y_train_pred = (y_train_prob > 0.5).astype(int)\n",
        "y_test_pred  = (y_test_prob  > 0.5).astype(int)\n",
        "\n",
        "# 2. Cálculo e exibição de acurácia\n",
        "print(\"\\n=== Avaliação LSTM ===\")\n",
        "print(\"Acurácia Treino:\", accuracy_score(y_train_LSTM, y_train_pred))\n",
        "print(\"Acurácia Teste:\",  accuracy_score(y_test_LSTM, y_test_pred))\n",
        "\n",
        "# 3. Relatório de classificação\n",
        "print(\"\\nClassification Report (Teste):\")\n",
        "print(classification_report(y_test_LSTM, y_test_pred, digits=4))\n",
        "\n",
        "# 4. Matriz de confusão (counts)\n",
        "cm = confusion_matrix(y_test_LSTM, y_test_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negativo (0)\", \"Positivo (1)\"])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "disp.plot(cmap='Blues', ax=ax, colorbar=False)\n",
        "ax.set_title(\"Matriz de Confusão — Teste\")\n",
        "plt.show()\n",
        "\n",
        "# 5. Matriz de confusão normalizada\n",
        "cm_norm = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
        "plt.title(\"Matriz de Confusão Normalizada\")\n",
        "plt.xlabel(\"Classe Predita\")\n",
        "plt.ylabel(\"Classe Verdadeira\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Cálculo de ROC AUC e Curva ROC\n",
        "roc_auc = roc_auc_score(y_test_LSTM, y_test_prob)\n",
        "fpr, tpr, _ = roc_curve(y_test_LSTM, y_test_prob)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0,1], [0,1], 'k--', linewidth=0.8)\n",
        "plt.title(\"Curva ROC — Teste\")\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Curva Precision–Recall\n",
        "precision, recall, _ = precision_recall_curve(y_test_LSTM, y_test_prob)\n",
        "pr_auc = sk_auc(recall, precision)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(recall, precision, label=f\"PR AUC = {pr_auc:.4f}\")\n",
        "plt.title(\"Curva Precision–Recall — Teste\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FyBaIgPd2d8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parte 7. Treinamento com early stopping\n",
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "history = model_LSTM.fit(\n",
        "    X_train_LSTM, y_train_LSTM,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "#Parte 8. Avaliação do modelo\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Predição em treino e teste\n",
        "y_train_pred = (model_LSTM.predict(X_train_LSTM) > 0.5).astype(int).flatten()\n",
        "y_test_pred  = (model_LSTM.predict(X_test_LSTM)  > 0.5).astype(int).flatten()\n",
        "\n",
        "# Exibe métricas\n",
        "print(\"\\n=== Avaliação LSTM ===\")\n",
        "print(\"Acurácia Treino:\", accuracy_score(y_train_LSTM, y_train_pred))\n",
        "print(\"Acurácia Teste:\",  accuracy_score(y_test_LSTM, y_test_pred))\n",
        "print(\"Matriz de Confusão (Teste):\\n\", confusion_matrix(y_test_LSTM, y_test_pred))\n",
        "\n",
        "#Parte 9. Aplicação ao DataFrame completo e correlação\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Verifica coluna 'headline'\n",
        "assert 'headline' in data_df.columns, \"Coluna 'headline' não encontrada em data_df.\"\n",
        "\n",
        "# Preprocessa textos e faz predição\n",
        "seq_data   = tokenizer.texts_to_sequences(data_df[\"headline\"].astype(str))\n",
        "X_seq_data = pad_sequences(seq_data, maxlen=50, padding='post', truncating='post')\n",
        "y_sentiment_pred = (model_LSTM.predict(X_seq_data) > 0.5).astype(int).flatten()\n",
        "\n",
        "# Inclui no DataFrame e calcula correlação\n",
        "data_df[\"sentiment_LSTM\"] = y_sentiment_pred\n",
        "correlacao = data_df[\"eventRet\"].corr(data_df[\"sentiment_LSTM\"])\n",
        "print(f\"Correlação entre sentimento LSTM e retorno: {correlacao:.4f}\")\n",
        "\n",
        "# Salvamento em CSV\n",
        "data_df.to_csv(\"/content/Step4_Resultado_LSTM.csv\", sep=\"|\", index=False)\n",
        "print(\"✅ Resultados salvos com sucesso.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OGUcGAKCnvR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Etapa 5 – Aplicação do modelo LSTM sobre data_df\n",
        "# Verifica se data_df possui coluna 'headline'\n",
        "assert 'headline' in data_df.columns, \"Coluna 'headline' não encontrada em data_df.\"\n",
        "\n",
        "# Preprocessamento\n",
        "seq_data = tokenizer.texts_to_sequences(data_df[\"headline\"].astype(str))\n",
        "X_seq_data = pad_sequences(seq_data, maxlen=50)\n",
        "\n",
        "# Predição\n",
        "y_sentiment_pred = (model_LSTM.predict(X_seq_data) > 0.5).astype(int).flatten()\n",
        "\n",
        "# Adiciona ao dataframe\n",
        "data_df[\"sentiment_LSTM\"] = y_sentiment_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "60qruaVg2zbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 6 – Correlação com retorno\n",
        "correlacao = data_df[\"eventRet\"].corr(data_df[\"sentiment_LSTM\"])\n",
        "print(f\"Correlação entre sentimento LSTM e retorno: {correlacao:.4f}\")\n"
      ],
      "metadata": {
        "id": "sq4mVFxM27iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Etapa 7 – Exportação final (opcional)\n",
        "data_df.to_csv(\"/content/Step4_Resultado_LSTM.csv\", sep=\"|\", index=False)\n",
        "print(\"✅ Resultados salvos com sucesso.\")"
      ],
      "metadata": {
        "id": "TsoxURKA3BM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k-HL-bNaVHZ"
      },
      "source": [
        "<a id='4.3'></a>\n",
        "## 4.3 - Modelo não supervisionado baseado em léxico (VADER)\n",
        "\n",
        "Modelo não supervisionado: Foi implementado um modelo de análise de sentimento baseado na técnica léxica VADER (Valence Aware Dictionary for Sentiment Reasoning), originalmente em inglês, adaptado para o contexto financeiro em português. Para isso, utilizou-se um dicionário de polaridades construído com apoio de modelos de linguagem generativos (ChatGPT-4 Mini e Gemini 2.5), contendo termos financeiros com valência atribuída manualmente e armazenada em formato JSON.\n",
        "Os valores de polaridade foram normalizados na escala de -4 a +4 e integrados ao léxico original do VADER. Com isso, o modelo passou a reconhecer e ponderar termos específicos do domínio financeiro, aumentando sua sensibilidade semântica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYkud4LxaVHZ"
      },
      "source": [
        "O VADER (Valence Aware Dictionary for Sentiment Reasoning) é um modelo pré-construído de análise de sentimentos incluído no pacote NLTK."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "0oaXOSejjh8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa o analisador de sentimento VADER\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Aplica o modelo padrão VADER na coluna de manchetes\n",
        "sentiments_data[\"sentiment_vader\"] = sentiments_data[\"headline\"].apply(\n",
        "    lambda x: sia.polarity_scores(str(x))[\"compound\"]\n",
        ")\n",
        "\n",
        "# Visualização\n",
        "print(sentiments_data[[\"headline\", \"sentiment_vader\"]].head())\n"
      ],
      "metadata": {
        "id": "LUYIoanTm-TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Baixar o VADER, caso ainda não tenha feito\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Carregar o VADER padrão\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Carregar termos do arquivo JSON\n",
        "with open('/content/TERMOS.json', encoding='utf-8') as f:\n",
        "    termos = json.load(f)\n",
        "\n",
        "# Converter para DataFrame para facilitar manipulação\n",
        "df_lexico = pd.DataFrame(termos)\n",
        "\n",
        "# Remover duplicatas baseadas no termo\n",
        "df_lexico = df_lexico.drop_duplicates(subset=\"termo\")\n",
        "\n",
        "# Construir o dicionário: termo → polaridade\n",
        "lexico_customizado = dict(zip(df_lexico['termo'], df_lexico['polaridade']))\n",
        "\n",
        "# Escalar os valores para mesma faixa do VADER (~ -4 a +4)\n",
        "max_pos = max([v for v in lexico_customizado.values() if v > 0])\n",
        "min_neg = min([v for v in lexico_customizado.values() if v < 0])\n",
        "\n",
        "lexico_ajustado = {}\n",
        "for termo, valor in lexico_customizado.items():\n",
        "    if valor > 0:\n",
        "        lexico_ajustado[termo] = valor / max_pos * 4\n",
        "    else:\n",
        "        lexico_ajustado[termo] = valor / abs(min_neg) * -4\n"
      ],
      "metadata": {
        "id": "rAM-2a7CwyC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Atualizar o léxico original do VADER\n",
        "sia.lexicon.update(lexico_ajustado)\n"
      ],
      "metadata": {
        "id": "fhAnesvfw9af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipJjv_e2aVHZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Aplicando o VADER nas manchetes do dataset de notícias\n",
        "sentiments_data['sentiment_lex'] = sentiments_data['headline'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "# Visualização dos resultados\n",
        "sentiments_data[['headline', 'sentiment_lex']].head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwyV7kVNaVHZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(sentiments_data['sentiment_lex'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribuição dos Sentimentos (Léxico Financeiro VADER)')\n",
        "plt.xlabel('Pontuação de Sentimento')\n",
        "plt.ylabel('Frequência')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e63Ll6QaVHZ"
      },
      "outputs": [],
      "source": [
        "corrlation = data_df['eventRet'].corr(data_df['sentiment_lex'])\n",
        "print(corrlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23kL3ImlaVHZ"
      },
      "outputs": [],
      "source": [
        "plt.scatter(data_df['sentiment_lex'],data_df['eventRet'], alpha=0.5)\n",
        "plt.title('Scatter Between Event return and sentiments-all data')\n",
        "plt.ylabel('Event Return')\n",
        "plt.xlabel('Sentiments')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualização dos dois principais tickers\n",
        "for ticker in tickers_top[:2]:\n",
        "    subset = data_df[data_df['ticker'] == ticker].dropna()\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    axs[0].scatter(subset['sentiment_lex'], subset['eventRet'], alpha=0.5)\n",
        "    axs[0].set_title(f'{ticker} - Lexico x Retorno')\n",
        "    axs[0].set_xlabel(\"Sentimento (Léxico)\")\n",
        "    axs[0].set_ylabel(\"Retorno\")\n",
        "\n",
        "    axs[1].scatter(subset['sentiment_LSTM'], subset['eventRet'], alpha=0.5)\n",
        "    axs[1].set_title(f'{ticker} - LSTM x Retorno')\n",
        "    axs[1].set_xlabel(\"Sentimento (LSTM)\")\n",
        "    axs[1].set_ylabel(\"Retorno\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "YCJAZbMf9nS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "040jZwYVaVHa"
      },
      "source": [
        "<a id='4.4'></a>\n",
        "## 4.4 Análise Exploratória e Comparação dos Modelos de Sentimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i8xLGx7aVHa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Carregando DataFrame consolidado\n",
        "data_df = pd.read_csv(\"/content/Step4_Resultado_LSTM.csv\", sep=\"|\")\n",
        "\n",
        "# Seleciona as colunas relevantes\n",
        "colunas_modelos = [\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment',\n",
        "    'eventRet'\n",
        "]\n",
        "\n",
        "# Remove valores ausentes\n",
        "df_corr = data_df[colunas_modelos].dropna()\n",
        "\n",
        "# Matriz de correlação\n",
        "correlacoes = df_corr.corr()\n",
        "\n",
        "# Visualização com heatmap\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(correlacoes[['eventRet']], annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Correlação entre Sentimentos e Retorno de Evento\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise de correlação por ticker\n",
        "corr_data = []\n",
        "\n",
        "for ticker in data_df['ticker'].dropna().unique():\n",
        "    subset = data_df[data_df['ticker'] == ticker].dropna(subset=colunas_modelos)\n",
        "    if subset.shape[0] >= 40:\n",
        "        linha = [ticker]\n",
        "        for col in colunas_modelos[:-1]:  # Exceto 'eventRet'\n",
        "            linha.append(subset['eventRet'].corr(subset[col]))\n",
        "        corr_data.append(linha)\n",
        "\n",
        "# Criação do DataFrame\n",
        "colunas_corr = ['ticker'] + [f'corr_{col}' for col in colunas_modelos[:-1]]\n",
        "corr_df = pd.DataFrame(corr_data, columns=colunas_corr).set_index('ticker')\n",
        "corr_df.head()\n"
      ],
      "metadata": {
        "id": "PDArQjSJ9WJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibe os 5 ativos com maior retorno absoluto para comparação\n",
        "tickers_top = corr_df['corr_sentiment_LSTM'].abs().sort_values(ascending=False).head(5).index.tolist()\n",
        "\n",
        "corr_df.loc[tickers_top].plot(kind='bar', figsize=(12, 6))\n",
        "plt.title(\"Correlação por Ticker - Modelos de Sentimento\")\n",
        "plt.ylabel(\"Correlação com Retorno de Evento\")\n",
        "plt.xlabel(\"Ticker\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R5fU1V7a9b1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lbw1AOq0_7e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVuyyCRoaVHb"
      },
      "source": [
        "<a id='5'></a>\n",
        "# 5. Estratégia de Negociação Baseada em Sentimento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install backtrader yfinance --quiet\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import backtrader as bt\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "GPhWPUDl-YWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05FJvHmHaVHb"
      },
      "source": [
        "Os dados de sentimento podem ser usados ​​de diferentes maneiras para a estratégia de negociação. As pontuações de sentimento podem ser usadas como um sinal direcional e, idealmente, criar uma carteira long-short, comprando as ações com pontuação positiva e vendendo as ações com pontuação negativa. Os sentimentos também podem ser usados ​​como recursos adicionais, além de outros recursos (como ações correlacionadas e indicadores técnicos), em um modelo de aprendizado supervisionado para prever o preço ou elaborar uma estratégia de negociação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1zAL4a5aVHb"
      },
      "source": [
        "Na estratégia de negociação neste estudo de caso, compramos e vendemos ações de acordo com os sentimentos atuais das ações:\n",
        "\n",
        "* Compre uma ação quando a mudança na pontuação de sentimento (pontuação de sentimento atual - pontuação de sentimento anterior) for maior que 0,5 e venda uma ação quando a mudança na pontuação de sentimento for menor que -0,5.\n",
        "* Além disso, verificamos a média móvel de 15 dias durante a compra e venda e compramos ou vendemos em uma unidade de 100.\n",
        "Obviamente, pode haver muitas maneiras de criar uma estratégia de negociação baseada em sentimentos, variando o limite ou alterando o número de unidades com base no dinheiro inicial disponível.\n",
        "\n",
        "Usamos sentimentos baseados em léxico para a estratégia de negociação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn1wgRt_aVHb"
      },
      "source": [
        "<a id='5.1'></a>\n",
        "## 5.1.  Definição da Estratégia com Backtrader\n",
        "Aqui, usamos o Backtrader, uma API baseada em Python para escrever e testar estratégias de negociação. O Backtrader permite que você se concentre em escrever estratégias de negociação, indicadores e analisadores reutilizáveis, em vez de ter que gastar tempo construindo infraestrutura. Temos uma estrutura conveniente para testar e escrever nossa estratégia de negociação. Usamos o código Quickstart na documentação (consulte https://www.backtrader.com/docu/quickstart/quickstart/ ) como base e o modificamos para incluir as pontuações de sentimento.\n",
        "\n",
        "Implementamos uma estratégia simples para comprar se a pontuação de sentimento do dia anterior aumentar em 0,5 em relação ao último dia e vender se diminuir em 0,5.\n",
        "\n",
        "A função a seguir contém duas classes:\n",
        "\n",
        "Sentimento:\n",
        "SentimentStrat: A função \"next\" desta classe implementa a estratégia de negociação real."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Coleta de Dados de Texto e Mercado\n",
        "A primeira etapa consistiu na coleta automatizada de manchetes de notícias relacionadas a quatro ativos do mercado acionário brasileiro: VALE3.SA, PETR4.SA, ITUB4.SA e BBAS3.SA. A extração foi realizada por meio de requisições ao serviço RSS do Google News, utilizando expressões regulares parametrizadas com múltiplas formas de referência a cada ticker. O período considerado foi de 1º de janeiro de 2025 a 4 de maio de 2025, com segmentação mensal para respeitar limites de requisição e evitar sobrecarga nos servidores."
      ],
      "metadata": {
        "id": "tjal_1BhPM_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTAÇÃO DE BIBLIOTECAS ===\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# === CONFIGURAÇÃO INICIAL ===\n",
        "tickers_termos = {\n",
        "    'VALE3.SA': ['VALE3', 'VALE3.SA', 'BVMF:VALE3'],\n",
        "    'PETR4.SA': ['PETR4', 'PETR4.SA', 'BVMF:PETR4'],\n",
        "    'ITUB4.SA': ['ITUB4', 'ITUB4.SA', 'BVMF:ITUB4'],\n",
        "    'BBAS3.SA': ['BBAS3', 'BBAS3.SA', 'BVMF:BBAS3']\n",
        "}\n",
        "\n",
        "# Período de coleta\n",
        "inicio = datetime.strptime('2025-01-01', '%Y-%m-%d')\n",
        "fim    = datetime.strptime('2025-05-04', '%Y-%m-%d')\n",
        "\n",
        "# Diretório de saída\n",
        "data_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "saida_dir = f\"noticias_json_{data_suffix}\"\n",
        "os.makedirs(saida_dir, exist_ok=True)\n",
        "\n",
        "# === GERA INTERVALOS MENSAIS ENTRE DUAS DATAS ===\n",
        "def gerar_periodos(start, end):\n",
        "    periodos = []\n",
        "    atual = start\n",
        "    while atual < end:\n",
        "        fim_mes = (atual + relativedelta(months=1)) - relativedelta(days=1)\n",
        "        if fim_mes > end:\n",
        "            fim_mes = end\n",
        "        periodos.append((atual, fim_mes))\n",
        "        atual = fim_mes + relativedelta(days=1)\n",
        "    return periodos\n",
        "\n",
        "# === COLETA DE NOTÍCIAS POR TICKER E PERÍODO ===\n",
        "def coletar_noticias(ticker, termos, dt_ini, dt_fim):\n",
        "    query = \"(\" + \" OR \".join([f'\"{t}\"' if ' ' in t else t for t in termos]) + \")\"\n",
        "    url = (\n",
        "        'https://news.google.com/rss/search'\n",
        "        f'?q={query}+after:{dt_ini.strftime(\"%Y-%m-%d\")}+before:{dt_fim.strftime(\"%Y-%m-%d\")}'\n",
        "        '&hl=pt-BR&gl=BR&ceid=BR:pt-419'\n",
        "    )\n",
        "\n",
        "    noticias = []\n",
        "    try:\n",
        "        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
        "        soup = BeautifulSoup(resp.content, 'xml')\n",
        "\n",
        "        for item in soup.find_all('item'):\n",
        "            pub = datetime.strptime(item.pubDate.text, '%a, %d %b %Y %H:%M:%S %Z')\n",
        "            if not (dt_ini <= pub <= dt_fim):\n",
        "                continue\n",
        "            titulo = item.title.text\n",
        "            link = item.link.text\n",
        "            resumo = BeautifulSoup(item.description.text, 'html.parser').get_text()\n",
        "            if any(term.lower() in (titulo + link).lower() for term in termos):\n",
        "                noticias.append({\n",
        "                    'ticker': ticker,\n",
        "                    'date': pub.strftime('%Y-%m-%d'),\n",
        "                    'time': pub.strftime('%H:%M'),\n",
        "                    'headline': titulo,\n",
        "                    'summary': resumo,\n",
        "                    'source': 'Google News RSS',\n",
        "                    'url': link,\n",
        "                    'language': 'pt',\n",
        "                    'scraped_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                })\n",
        "    except Exception as e:\n",
        "        print(f\"[ERRO] {ticker} ({dt_ini.date()} a {dt_fim.date()}): {e}\")\n",
        "    return noticias\n",
        "\n",
        "# === EXECUÇÃO DA COLETA ===\n",
        "periodos = gerar_periodos(inicio, fim)\n",
        "\n",
        "for ticker, termos in tickers_termos.items():\n",
        "    todas = []\n",
        "    for dt_ini, dt_fim in periodos:\n",
        "        todas += coletar_noticias(ticker, termos, dt_ini, dt_fim)\n",
        "\n",
        "    if todas:\n",
        "        path_json = os.path.join(saida_dir, f\"{ticker}_noticias_{data_suffix}.json\")\n",
        "        with open(path_json, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(todas, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"[✔️] {ticker}: {len(todas)} notícias salvas.\")\n",
        "\n",
        "# === COMPACTAÇÃO FINAL ===\n",
        "zip_path = f\"noticias_tickers_{data_suffix}.zip\"\n",
        "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for arq in os.listdir(saida_dir):\n",
        "        zipf.write(os.path.join(saida_dir, arq), arcname=arq)\n",
        "\n",
        "print(f\"\\n Arquivo ZIP final criado: {zip_path}\")\n"
      ],
      "metadata": {
        "id": "9eedoLP6_Fz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como resultado, foram coletadas:\n",
        "* 367 manchetes para VALE3.SA\n",
        "* 384 manchetes para PETR4.SA\n",
        "* 245 manchetes para ITUB4.SA\n",
        "* 301 manchetes para BBAS3.SA\n",
        "\n",
        "Cada registro inclui data e hora da publicação, manchete, resumo da notícia, fonte, URL e idioma detectado. As notícias foram armazenadas em arquivos JSON e posteriormente estruturadas em um DataFrame padronizado para análise.\n"
      ],
      "metadata": {
        "id": "L2cK0MJdPTKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def calcular_polaridade_continua(model_id, n_classes, textos):\n",
        "    \"\"\"\n",
        "    Aplica modelo Hugging Face e transforma saída de classificação em valor contínuo entre -1 e +1.\n",
        "    \"\"\"\n",
        "    # Define pipeline de sentimento com saída de probabilidade por classe\n",
        "    classifier = pipeline(\"text-classification\", model=model_id, return_all_scores=True, truncation=True)\n",
        "\n",
        "    resultados = []\n",
        "\n",
        "    for texto in tqdm(textos, desc=f\"Analisando com {model_id}\"):\n",
        "        try:\n",
        "            # Obtém as probabilidades para cada classe\n",
        "            scores = classifier(str(texto))[0]\n",
        "\n",
        "            # Mapeia classes padrão: assume 3 classes [NEG, NEU, POS]\n",
        "            if n_classes == 3:\n",
        "                probs = {s['label'].lower(): s['score'] for s in scores}\n",
        "\n",
        "                # Normaliza nomes típicos de labels para consistência\n",
        "                neg = next((v for k, v in probs.items() if 'neg' in k), 0)\n",
        "                neu = next((v for k, v in probs.items() if 'neu' in k), 0)\n",
        "                pos = next((v for k, v in probs.items() if 'pos' in k), 0)\n",
        "\n",
        "                # Calcula escore contínuo: -1*neg + 0*neu + 1*pos = pos - neg\n",
        "                score = pos - neg\n",
        "            else:\n",
        "                raise ValueError(\"Modelo com número de classes diferente de 3 não suportado.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERRO interno] Texto: {texto[:50]}... -> {e}\")\n",
        "            score = 0.0  # fallback neutro\n",
        "\n",
        "        resultados.append(score)\n",
        "\n",
        "    return resultados\n"
      ],
      "metadata": {
        "id": "kZFfDKJaITYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Caminho para o arquivo ZIP gerado anteriormente\n",
        "zip_path = \"/content/noticias_tickers_20250504_203905.zip\"\n",
        "extract_path = \"/content/noticias_2025\"\n",
        "\n",
        "# Extrai o conteúdo do ZIP\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "with ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Consolida todos os arquivos JSON em um único DataFrame\n",
        "noticias_lista = []\n",
        "for arquivo in os.listdir(extract_path):\n",
        "    if arquivo.endswith(\".json\"):\n",
        "        caminho = os.path.join(extract_path, arquivo)\n",
        "        with open(caminho, encoding=\"utf-8\") as f:\n",
        "            dados = json.load(f)\n",
        "            noticias_lista.extend(dados)\n",
        "\n",
        "# Cria DataFrame único com as notícias\n",
        "df_noticias = pd.DataFrame(noticias_lista)\n",
        "df_noticias['date'] = pd.to_datetime(df_noticias['date']).dt.date  # normaliza data\n",
        "df_noticias = df_noticias[['date', 'ticker', 'headline']].dropna()\n",
        "\n",
        "# Visualização\n",
        "print(f\"[✔️] Total de registros carregados: {len(df_noticias)}\")\n",
        "df_noticias.head()\n"
      ],
      "metadata": {
        "id": "CfR360GDB4g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rd57375kPgP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamento e Consolidação dos Arquivos de Notícias"
      ],
      "metadata": {
        "id": "J8GSR0nQB6x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicar os Modelos de Sentimento (Já Treinados)"
      ],
      "metadata": {
        "id": "ECTFbJbDCAzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Classificação de Sentimento com Modelos NLP\n",
        "\n",
        "\n",
        "As manchetes foram submetidas aos cinco modelos de análise de sentimento descritos anteriormente:\n",
        "\n",
        "LSTM supervisionado (baseado em retorno de evento)\n",
        "\n",
        "VADER com léxico financeiro em português\n",
        "\n",
        "FinBERT_PT_BR\n",
        "\n",
        "FinBERT_Turing\n",
        "\n",
        "Multilingual_Sentiment\n",
        "\n",
        "Todos os modelos foram aplicados com saída contínua no intervalo [–1, +1], por meio de transformação dos logits com softmax (nos modelos Hugging Face) ou diretamente por cálculo de escore composto (no caso do VADER)."
      ],
      "metadata": {
        "id": "QTYrb01iPvyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === GARANTE QUE AS COLUNAS ORIGINAIS NÃO SERÃO PERDIDAS ===\n",
        "colunas_originais = df_noticias.columns.tolist()\n",
        "\n",
        "# === MODELO LSTM ===\n",
        "print(\" Aplicando modelo LSTM...\")\n",
        "seq = tokenizer.texts_to_sequences(df_noticias['headline'].astype(str))\n",
        "X_seq = pad_sequences(seq, maxlen=50)\n",
        "df_noticias['sentiment_LSTM_raw'] = model_LSTM.predict(X_seq, verbose=0).flatten()\n",
        "df_noticias['sentiment_LSTM'] = (2 * df_noticias['sentiment_LSTM_raw']) - 1  # Normaliza para [-1, 1]\n",
        "\n",
        "# === MODELO VADER (LÉXICO FINANCEIRO) ===\n",
        "print(\" Aplicando modelo VADER com léxico financeiro...\")\n",
        "df_noticias['sentiment_lex'] = df_noticias['headline'].apply(\n",
        "    lambda x: sia.polarity_scores(str(x))['compound']\n",
        ")\n",
        "\n",
        "# === MODELOS HUGGING FACE COM TRATAMENTO DE INVERSÃO ===\n",
        "modelos_hf = {\n",
        "    \"FinBERT_PT_BR\": {\n",
        "        \"repo\": \"lucas-leme/FinBERT-PT-BR\",\n",
        "        \"n_classes\": 3,\n",
        "        \"inverter_escala\": True  # ← Inverter resultado\n",
        "    },\n",
        "    \"FinBERT_Turing\": {\n",
        "        \"repo\": \"turing-usp/FinBertPTBR\",\n",
        "        \"n_classes\": 3,\n",
        "        \"inverter_escala\": True  # ← Inverter resultado\n",
        "    },\n",
        "    \"Multilingual_Sentiment\": {\n",
        "        \"repo\": \"tabularisai/multilingual-sentiment-analysis\",\n",
        "        \"n_classes\": 3,\n",
        "        \"inverter_escala\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "for nome_modelo, config in modelos_hf.items():\n",
        "    print(f\" Aplicando {nome_modelo}...\")\n",
        "    try:\n",
        "        coluna_saida = f\"sentiment_{nome_modelo}\"\n",
        "        valores = calcular_polaridade_continua(\n",
        "            model_id=config[\"repo\"],\n",
        "            n_classes=config[\"n_classes\"],\n",
        "            textos=df_noticias[\"headline\"]\n",
        "        )\n",
        "\n",
        "        # Inversão se necessário\n",
        "        if config.get(\"inverter_escala\", False):\n",
        "            valores = [-1 * v for v in valores]\n",
        "\n",
        "        df_noticias[coluna_saida] = valores\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERRO] {nome_modelo}: {e}\")\n",
        "\n",
        "# === VERIFICAÇÃO FINAL ===\n",
        "print(\"Colunas após aplicação de todos os modelos:\")\n",
        "print(df_noticias.columns.tolist())\n"
      ],
      "metadata": {
        "id": "isJLz3NjwBtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregação Diária por Ticker"
      ],
      "metadata": {
        "id": "qsORKP3MCURL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Validação: imprime colunas disponíveis\n",
        "print(\"Colunas disponíveis em df_noticias:\", df_noticias.columns.tolist())\n"
      ],
      "metadata": {
        "id": "sjVrVTp7wcvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Agrupamento por data e ticker, ordenando por ordem cronológica para manter consistência\n",
        "agrupado = df_noticias.sort_values(['date', 'ticker']).groupby(['date', 'ticker'])\n",
        "\n",
        "# Agregação dos sentimentos por média, e preservação da primeira notícia e resumo\n",
        "df_diario = agrupado.agg({\n",
        "    'headline': 'first',\n",
        "    'sentiment_lex': 'mean',\n",
        "    'sentiment_LSTM': 'mean',\n",
        "    'sentiment_FinBERT_PT_BR': 'mean',\n",
        "    'sentiment_FinBERT_Turing': 'mean',\n",
        "    'sentiment_Multilingual_Sentiment': 'mean',\n",
        "}).reset_index()\n",
        "\n",
        "# Visualização e confirmação\n",
        "print(f\"[✔️] df_diario gerado com {df_diario.shape[0]} linhas e {df_diario.shape[1]} colunas.\")\n",
        "display(df_diario.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OmPhD7txwgCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Verifica estrutura do DataFrame\n",
        "colunas = df_diario.columns.tolist()\n",
        "tickers = df_diario['ticker'].unique()\n",
        "data_inicio = df_diario['date'].min()\n",
        "data_fim = df_diario['date'].max()\n",
        "\n",
        "# 2. Calcula estatísticas (mínimo, máximo e média) por modelo\n",
        "estatisticas = df_diario[[\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment'\n",
        "]].describe().loc[['min', 'max', 'mean']].round(3)\n",
        "\n",
        "# 3. Mostra resumo textual\n",
        "display(Markdown(f\"\"\"\n",
        "### 📄 Estrutura do DataFrame\n",
        "\n",
        "**Colunas presentes:**\n",
        "`{', '.join(colunas)}`\n",
        "\n",
        "**Tickers únicos encontrados:**\n",
        "{', '.join(tickers)}\n",
        "\n",
        "**Intervalo de datas disponíveis:**\n",
        "De **{data_inicio}** até **{data_fim}**\n",
        "\"\"\"))\n",
        "\n",
        "# 4. Mostra estatísticas com formatação\n",
        "cmap = sns.light_palette(\"green\", as_cmap=True)\n",
        "\n",
        "display(\n",
        "    estatisticas.style\n",
        "        .background_gradient(cmap=cmap)\n",
        "        .format(precision=3)\n",
        "        .set_caption(\"📊 Estatísticas dos Sentimentos (mín, máx, média)\")\n",
        "        .set_properties(**{'font-size': '10pt', 'width': '80px'})\n",
        ")\n"
      ],
      "metadata": {
        "id": "OSPlG6imC4m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Lista de tickers\n",
        "tickers = ['BBAS3.SA', 'ITUB4.SA', 'PETR4.SA', 'VALE3.SA']\n",
        "\n",
        "# Colunas de sentimento\n",
        "colunas_sentimentos = [\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment'\n",
        "]\n",
        "\n",
        "# Armazena as tabelas formatadas\n",
        "html_tabelas = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    df_ticker = df_diario[df_diario['ticker'] == ticker]\n",
        "\n",
        "    if len(df_ticker) < 2:\n",
        "        continue\n",
        "\n",
        "    matriz_corr = df_ticker[colunas_sentimentos].corr(method='pearson')\n",
        "\n",
        "    styler = matriz_corr.style\\\n",
        "        .background_gradient(cmap='coolwarm')\\\n",
        "        .format(precision=2)\\\n",
        "        .set_caption(f\"<b>{ticker}</b>\")\\\n",
        "        .set_properties(**{\n",
        "            'font-size': '9pt',\n",
        "            'width': '60px'\n",
        "        })\n",
        "\n",
        "    html_tabelas.append(styler.to_html())\n",
        "\n",
        "# Cria layout horizontal com CSS flexbox\n",
        "html_final = f\"\"\"\n",
        "<div style=\"display: flex; gap: 20px;\">\n",
        "    {''.join(f'<div>{t}</div>' for t in html_tabelas)}\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Exibe todas as matrizes lado a lado\n",
        "display(HTML(html_final))\n"
      ],
      "metadata": {
        "id": "DMbCfE0HKceM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")  # fundo com grid discreto\n",
        "plt.rcParams.update({\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.titleweight': 'bold',\n",
        "    'axes.labelsize': 12,\n",
        "    'xtick.labelsize': 10,\n",
        "    'ytick.labelsize': 10,\n",
        "    'legend.fontsize': 10,\n",
        "    'lines.linewidth': 2,\n",
        "    'grid.alpha': 0.6,\n",
        "    'grid.linestyle': '--',\n",
        "    'figure.figsize': (14, 6)\n",
        "})\n",
        "\n",
        "# Ativo em foco\n",
        "ticker_exemplo = 'VALE3.SA'\n",
        "df_exemplo = df_diario[df_diario['ticker'] == ticker_exemplo].copy()\n",
        "\n",
        "# Média móvel para suavização dos dados\n",
        "window = 5\n",
        "for col in [\n",
        "    'sentiment_lex', 'sentiment_LSTM', 'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing', 'sentiment_Multilingual_Sentiment'\n",
        "]:\n",
        "    df_exemplo[col + '_smoothed'] = df_exemplo[col].rolling(window=window, center=True).mean()\n",
        "\n",
        "# Cores elegantes e consistentes\n",
        "cores = {\n",
        "    'sentiment_lex_smoothed': '#1f77b4',\n",
        "    'sentiment_LSTM_smoothed': '#ff7f0e',\n",
        "    'sentiment_FinBERT_PT_BR_smoothed': '#2ca02c',\n",
        "    'sentiment_FinBERT_Turing_smoothed': '#d62728',\n",
        "    'sentiment_Multilingual_Sentiment_smoothed': '#9467bd'\n",
        "}\n",
        "\n",
        "# Criação da figura\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plotagem das curvas suavizadas\n",
        "for col, cor in cores.items():\n",
        "    ax.plot(df_exemplo['date'], df_exemplo[col], label=col.replace('_smoothed', ''), color=cor)\n",
        "\n",
        "# Ajustes finais no layout\n",
        "ax.set_title(f\"Evolução dos Sentimentos – {ticker_exemplo}\")\n",
        "ax.set_xlabel(\"Data\")\n",
        "ax.set_ylabel(\"Sentimento Médio Diário\")\n",
        "ax.legend(frameon=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bF_3DptlM0bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Etapa 2: Execução da Estratégia Individual por Modelo de Sentimento\n",
        "\n",
        "### Tentei incluir a estratégia de média móvel, mas não consegui.\n",
        "\n",
        "Será realizada a simulação da estratégia para **cada coluna de sentimento** (`sentiment_lex`, `sentiment_LSTM`, etc.) **individualmente**. Como o intervalo de tempo da análise é curto (`2025-01-01` a `2025-05-04` ≈ 4 meses), utilizar **apenas a média móvel de 15 dias (SMA15)** pode limitar a responsividade da estratégia. Adotamos então variação da estratégia com múltiplas médias móveis:\n",
        "\n",
        "| Nome      | Período | Finalidade                              |\n",
        "| --------- | ------- | --------------------------------------- |\n",
        "| **SMA5**  | 5 dias  | Curto prazo, mais sensível a reversões  |\n",
        "| **SMA10** | 10 dias | Curto/médio prazo, menos ruído que SMA5 |\n",
        "| **SMA15** | 15 dias | Referência padrão de tendência          |\n",
        "| **SMA20** | 20 dias | Média mais \"suave\", evita sinais falsos |\n",
        "\n",
        "---\n",
        "\n",
        "### Estratégia por Média Móvel\n",
        "\n",
        "Para cada uma dessas médias móveis, aplicaremos:\n",
        "\n",
        "#### **Critério de Compra**\n",
        "\n",
        "* > Compra: Δsentimento > 0.5 e Preço atual > média móvel (SMAx)\n",
        "* > Venda: Δsentimento < -0.5 e Preço atual < média móvel (SMAx)\n",
        "\n",
        "\n",
        "### Execuções planejadas\n",
        "\n",
        "Para **cada modelo de sentimento** (`sentiment_lex`, `sentiment_LSTM`, etc.), a estratégia será executada **para cada média móvel separadamente**, totalizando:\n",
        "\n",
        "```\n",
        "5 modelos × 4 médias móveis = 20 simulações\n",
        "```\n",
        "\n",
        "Cada simulação terá:\n",
        "\n",
        "* Valor inicial: R\\$ 100.000\n",
        "* Lote fixo: 100 ações\n",
        "* Período: 2025-01-01 a 2025-05-04\n"
      ],
      "metadata": {
        "id": "-aVkDKdVD1jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Coleta dos preços históricos com médias móveis\n",
        "\n",
        "Código para coletar os preços históricos de fechamento via `yfinance`, calcular as médias móveis e deixar tudo pronto para cruzar com o DataFrame de sentimentos por ticker e data:"
      ],
      "metadata": {
        "id": "-M-HyBpSPJ9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTAÇÃO DE BIBLIOTECAS\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "\n",
        "# PARÂMETROS DE CONFIGURAÇÃO\n",
        "\n",
        "# Tickers brasileiros analisados\n",
        "tickers = ['VALE3.SA', 'PETR4.SA', 'ITUB4.SA', 'BBAS3.SA']\n",
        "\n",
        "# Intervalo de datas da análise\n",
        "start = '2025-01-01'\n",
        "end   = '2025-05-04'\n",
        "\n",
        "# Diretório de saída\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# MAPA DOS DIAS DA SEMANA EM PORTUGUÊS\n",
        "\n",
        "dias_semana_pt = {\n",
        "    'Monday': 'Segunda-feira',\n",
        "    'Tuesday': 'Terça-feira',\n",
        "    'Wednesday': 'Quarta-feira',\n",
        "    'Thursday': 'Quinta-feira',\n",
        "    'Friday': 'Sexta-feira',\n",
        "    'Saturday': 'Sábado',\n",
        "    'Sunday': 'Domingo'\n",
        "}\n",
        "\n",
        "# COLETA DOS DADOS HISTÓRICOS COM TRATAMENTO\n",
        "\n",
        "dados_completos = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    print(f\" Coletando dados de: {ticker}\")\n",
        "    try:\n",
        "        ticker_yf = yf.Ticker(ticker)\n",
        "        dados = ticker_yf.history(start=start, end=end)\n",
        "\n",
        "        # Validação: ignorar tickers com dados vazios\n",
        "        if dados.empty:\n",
        "            print(f\" Dados indisponíveis para {ticker}, ignorado.\")\n",
        "            continue\n",
        "\n",
        "        # Resetar índice (Date como coluna)\n",
        "        dados = dados.reset_index()\n",
        "\n",
        "        # Garantir consistência de datas e tipos\n",
        "        dados['Date'] = pd.to_datetime(dados['Date'], errors='coerce')\n",
        "        dados = dados.dropna(subset=['Date'])\n",
        "\n",
        "        # Formatação de colunas auxiliares\n",
        "        dados['Data'] = dados['Date'].dt.strftime('%Y-%m-%d')\n",
        "        dados['Hora'] = dados['Date'].dt.strftime('%H:%M:%S')\n",
        "        dados['Dia_da_semana'] = dados['Date'].dt.day_name().map(dias_semana_pt)\n",
        "        dados['Ticker'] = ticker\n",
        "\n",
        "        # Seleção e ordenação de colunas\n",
        "        dados_formatado = dados[[\n",
        "            'Data', 'Dia_da_semana', 'Open', 'High', 'Low', 'Close',\n",
        "            'Volume', 'Dividends', 'Stock Splits', 'Ticker'\n",
        "        ]]\n",
        "\n",
        "        dados_completos.append(dados_formatado)\n",
        "        print(f\" {ticker}: {dados_formatado.shape[0]} registros coletados.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Erro ao coletar dados de {ticker}: {e}\")\n",
        "\n",
        "\n",
        "# CONSOLIDAÇÃO E SALVAMENTO DOS RESULTADOS\n",
        "\n",
        "if dados_completos:\n",
        "    df_final = pd.concat(dados_completos, ignore_index=True)\n",
        "    df_final = df_final.sort_values(by=['Ticker', 'Data'])\n",
        "\n",
        "    # Caminhos de saída\n",
        "    caminho_csv   = \"data/ReturnData_2025.csv\"\n",
        "    caminho_excel = \"data/ReturnData_2025.xlsx\"\n",
        "\n",
        "    # Salvamento em formatos compatíveis\n",
        "    df_final.to_csv(caminho_csv, sep='|', index=False)\n",
        "    df_final.to_excel(caminho_excel, index=False, engine='openpyxl')\n",
        "\n",
        "    print(f\"\\n Arquivos salvos com sucesso:\")\n",
        "    print(f\"  - CSV: {caminho_csv}\")\n",
        "    print(f\"  - Excel: {caminho_excel}\")\n",
        "\n",
        "    # Exibição interativa no Colab\n",
        "    display(data_table.DataTable(df_final))\n",
        "else:\n",
        "    print(\" Nenhum dado foi coletado com sucesso.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ghxl3Th-AACX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cálculo das Médias Móveis\n",
        "\n",
        "Abaixo está o código para adicionar colunas de médias móveis ao DataFrame `df_final`, para cada `Ticker` individualmente, usando os valores da coluna `Close`.\n",
        "\n",
        "Médias móveis incluídas:\n",
        "\n",
        "* SMA5: 5 dias (curto prazo)\n",
        "* SMA10: 10 dias\n",
        "* SMA15: 15 dias (padrão da estratégia)\n",
        "* SMA20: 20 dias (suavização maior)\n",
        "\n"
      ],
      "metadata": {
        "id": "5FuSOo64Rnfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÁLCULO DAS MÉDIAS MÓVEIS POR TICKER\n",
        "\n",
        "# Cópia de segurança do DataFrame original\n",
        "df_mm = df_final.copy()\n",
        "\n",
        "# Converte a coluna 'Data' para datetime (caso necessário)\n",
        "df_mm['Data'] = pd.to_datetime(df_mm['Data'], format='%Y-%m-%d', errors='coerce')\n",
        "\n",
        "# Ordena corretamente\n",
        "df_mm = df_mm.sort_values(by=['Ticker', 'Data'])\n",
        "\n",
        "# Cálculo das médias móveis por ticker\n",
        "for window in [5, 10, 15, 20]:\n",
        "    nome_coluna = f'SMA{window}'\n",
        "    df_mm[nome_coluna] = (\n",
        "        df_mm.groupby('Ticker')['Close']\n",
        "        .transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
        "    )\n",
        "\n",
        "# Verificação\n",
        "print(f\" Médias móveis calculadas com sucesso para {df_mm['Ticker'].nunique()} ativos.\")\n",
        "display(data_table.DataTable(df_mm.tail(10)))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sBL5wcYlRmwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Colunas disponíveis em df_diario:\")\n",
        "print(df_diario.columns.tolist())"
      ],
      "metadata": {
        "id": "937LX2pucy9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Colunas disponíveis em dados_formatado:\")\n",
        "print(dados_formatado.columns.tolist())"
      ],
      "metadata": {
        "id": "7uMgz654yGU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# --- 1. Geração de todas as datas no intervalo ---\n",
        "inicio = date(2025, 1, 1)\n",
        "fim = date(2025, 5, 4)\n",
        "todas_datas = pd.date_range(start=inicio, end=fim, freq='D').date\n",
        "\n",
        "# --- 2. Tickers disponíveis ---\n",
        "tickers = dados_formatado['ticker'].unique().tolist()\n",
        "\n",
        "# --- 3. Produto cartesiano: todos os dias para cada ticker ---\n",
        "base_completa = pd.MultiIndex.from_product(\n",
        "    [todas_datas, tickers],\n",
        "    names=['date', 'ticker']\n",
        ").to_frame(index=False)\n",
        "\n",
        "# --- 4. Junção com base de preços ---\n",
        "df_precos = dados_formatado.copy()\n",
        "df_precos['date'] = pd.to_datetime(df_precos['date']).dt.date\n",
        "merged_1 = pd.merge(base_completa, df_precos, how='left', on=['date', 'ticker'])\n",
        "\n",
        "# --- 5. Junção com base de sentimentos ---\n",
        "df_diario['date'] = pd.to_datetime(df_diario['date']).dt.date\n",
        "merged_2 = pd.merge(merged_1, df_diario, how='left', on=['date', 'ticker'])\n",
        "\n",
        "# --- 6. Atualização de Dia da Semana com nomes em português ---\n",
        "dias_pt = {\n",
        "    'Monday': 'Segunda-feira',\n",
        "    'Tuesday': 'Terça-feira',\n",
        "    'Wednesday': 'Quarta-feira',\n",
        "    'Thursday': 'Quinta-feira',\n",
        "    'Friday': 'Sexta-feira',\n",
        "    'Saturday': 'Sábado',\n",
        "    'Sunday': 'Domingo'\n",
        "}\n",
        "merged_2['Dia_da_semana'] = pd.to_datetime(merged_2['date']).dt.day_name().map(dias_pt)\n",
        "\n",
        "# --- 7. Organização final ---\n",
        "merged_2 = merged_2.sort_values(by=['ticker', 'date']).reset_index(drop=True)\n",
        "\n",
        "# --- 8. Estatísticas ---\n",
        "print(\"✅ Base unificada (preço + sentimento) criada com todas as datas.\")\n",
        "print(f\"Total de linhas: {merged_2.shape[0]}\")\n",
        "print(f\"Período coberto: {merged_2['date'].min()} até {merged_2['date'].max()}\")\n",
        "print(f\"Colunas disponíveis: {merged_2.columns.tolist()}\")\n",
        "\n",
        "# --- 9. Visualização ---\n",
        "display(merged_2.head(5))\n",
        "\n",
        "# --- 10. Salvamento final ---\n",
        "merged_2.to_csv(\"data/BaseCompleta_2025.csv\", sep='|', index=False)\n",
        "merged_2.to_excel(\"data/BaseCompleta_2025.xlsx\", index=False)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FcnT-JbOzPKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import backtrader as bt\n",
        "\n",
        "class SentimentStrat(bt.Strategy):\n",
        "    params = (\n",
        "        ('sentiment_dict', None),\n",
        "        ('sma_col', 'SMA15')\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        self.order = None\n",
        "        self.dataclose = self.datas[0].close\n",
        "        self.dates = self.datas[0].datetime\n",
        "\n",
        "    def next(self):\n",
        "        current_date = bt.num2date(self.dates[0]).date()\n",
        "        sentiment_today = self.params.sentiment_dict.get(current_date)\n",
        "        sentiment_yesterday = self.params.sentiment_dict.get(current_date - pd.Timedelta(days=1))\n",
        "\n",
        "        if sentiment_today is None or sentiment_yesterday is None:\n",
        "            return  # Não há dado de sentimento suficiente\n",
        "\n",
        "        delta_sentiment = sentiment_today - sentiment_yesterday\n",
        "        sma_value = getattr(self.datas[0], self.params.sma_col, None)[0]\n",
        "\n",
        "        if sma_value is None:\n",
        "            return  # Não há SMA suficiente\n",
        "\n",
        "        if not self.position:\n",
        "            if delta_sentiment > 0.5 and self.dataclose[0] > sma_value:\n",
        "                self.order = self.buy()\n",
        "        else:\n",
        "            if delta_sentiment < -0.5 and self.dataclose[0] < sma_value:\n",
        "                self.order = self.sell()\n"
      ],
      "metadata": {
        "id": "ueIt9s26nq2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import backtrader as bt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class SentimentStrat(bt.Strategy):\n",
        "    params = (\n",
        "        ('sentiment_dict', {}),\n",
        "        ('sma_col', 'sma'),\n",
        "    )\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dataclose = self.datas[0].close\n",
        "        self.order = None\n",
        "        self.sentiment = self.p.sentiment_dict\n",
        "        self.sma = self.datas[0].sma\n",
        "\n",
        "    def next(self):\n",
        "        data_atual = self.datas[0].datetime.date(0)\n",
        "\n",
        "        if data_atual not in self.sentiment:\n",
        "            return\n",
        "\n",
        "        sentimento_hoje = self.sentiment.get(data_atual, 0)\n",
        "        sentimento_ontem = self.sentiment.get(\n",
        "            data_atual - pd.Timedelta(days=1), 0)\n",
        "        delta_sent = sentimento_hoje - sentimento_ontem\n",
        "\n",
        "        if self.order:\n",
        "            return\n",
        "\n",
        "        if not self.position:\n",
        "            if delta_sent > 0.5 and self.dataclose[0] > self.sma[0]:\n",
        "                self.order = self.buy(size=100)\n",
        "        else:\n",
        "            if delta_sent < -0.5 and self.dataclose[0] < self.sma[0]:\n",
        "                self.order = self.sell(size=100)\n",
        "\n",
        "\n",
        "class PandasSentiment(bt.feeds.PandasData):\n",
        "    lines = ('sma',)\n",
        "    params = (\n",
        "        ('datetime', None),\n",
        "        ('open', 'Open'),\n",
        "        ('high', 'High'),\n",
        "        ('low', 'Low'),\n",
        "        ('close', 'Close'),\n",
        "        ('volume', 'Volume'),\n",
        "        ('openinterest', -1),\n",
        "        ('sma', -1),  # -1 = procura uma coluna com esse nome\n",
        "    )\n",
        "\n",
        "\n",
        "def run_strategy(ticker, start, end, sentiment_series, sma_col='SMA15', plot=False):\n",
        "    print(f\"Executando estratégia para: {ticker} | SMA: {sma_col}\")\n",
        "\n",
        "    janela = int(sma_col.replace('SMA', ''))\n",
        "\n",
        "    df_price = yf.download(ticker, start=start, end=end)\n",
        "    if df_price.empty:\n",
        "        print(f\"[ERRO] Dados de preço vazios para {ticker}.\")\n",
        "        return None\n",
        "\n",
        "   # **Change 1:** Check if columns is MultiIndex, and if so, convert to single level\n",
        "    if isinstance(df_price.columns, pd.MultiIndex):\n",
        "        df_price.columns = df_price.columns.get_level_values(0)\n",
        "\n",
        "    df_price['sma'] = df_price['Close'].rolling(window=janela).mean()\n",
        "    df_price = df_price[['Open', 'High', 'Low', 'Close', 'Volume', 'sma']].dropna()\n",
        "    df_price.index.name = 'datetime'\n",
        "\n",
        "    # **Change 2 (removed):** Removed converting column names to strings as it's unnecessary after flattening the MultiIndex\n",
        "\n",
        "    data_bt = PandasSentiment(dataname=df_price)\n",
        "\n",
        "    cerebro = bt.Cerebro()\n",
        "    cerebro.adddata(data_bt)\n",
        "    cerebro.addstrategy(SentimentStrat, sentiment_dict=sentiment_series)\n",
        "    cerebro.broker.setcash(100000.0)\n",
        "    cerebro.addsizer(bt.sizers.FixedSize, stake=100)\n",
        "\n",
        "    valor_inicial = cerebro.broker.getvalue()\n",
        "    cerebro.run()\n",
        "    valor_final = cerebro.broker.getvalue()\n",
        "\n",
        "    if plot:\n",
        "        cerebro.plot(volume=False, iplot=False)\n",
        "\n",
        "    return {\n",
        "        'ticker': ticker,\n",
        "        'sma': sma_col,\n",
        "        'valor_inicial': valor_inicial,\n",
        "        'valor_final': valor_final,\n",
        "        'lucro': valor_final - valor_inicial,\n",
        "        'lucro_pct': (valor_final - valor_inicial) / valor_inicial * 100\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ix5GbCKlnxcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# === PARÂMETROS ===\n",
        "ticker = 'VALE3.SA'\n",
        "modelos_sentimento = [\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment',\n",
        "    'sentiment_random'  # modelo de benchmark aleatório\n",
        "]\n",
        "capital_inicial = 100_000\n",
        "taxa_b3 = 0.0003  # 0,03%\n",
        "\n",
        "# === PERÍODO ===\n",
        "datas_validas = df_diario[df_diario['ticker'] == ticker]['date'].dropna()\n",
        "inicio = pd.to_datetime(datas_validas.min()).strftime('%Y-%m-%d')\n",
        "fim    = pd.to_datetime(datas_validas.max()).strftime('%Y-%m-%d')\n",
        "print(f\"📅 Período: {inicio} a {fim}\")\n",
        "\n",
        "# === PREÇOS ===\n",
        "df_price = yf.download(ticker, start=inicio, end=fim, progress=False)\n",
        "if isinstance(df_price.columns, pd.MultiIndex):\n",
        "    df_price.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df_price.columns]\n",
        "df_price = df_price.reset_index()\n",
        "df_price['date'] = pd.to_datetime(df_price['Date']).dt.date\n",
        "col_close = f'Close_{ticker}' if f'Close_{ticker}' in df_price.columns else 'Close'\n",
        "df_price['close'] = df_price[col_close]\n",
        "\n",
        "# === DIRETÓRIO ===\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# === RESULTADOS GERAIS ===\n",
        "resultados = []\n",
        "curvas = {}\n",
        "\n",
        "for modelo_sent in modelos_sentimento:\n",
        "    print(f\"\\n▶️ Modelo proporcional: {modelo_sent}\")\n",
        "\n",
        "    if modelo_sent == 'sentiment_random':\n",
        "        # Gera valores aleatórios entre -1 e 1\n",
        "        df_sent = pd.DataFrame({\n",
        "            'date': df_price['date'],\n",
        "            'sentiment_random': np.random.uniform(-1, 1, size=len(df_price))\n",
        "        })\n",
        "    else:\n",
        "        df_sent = df_diario[df_diario['ticker'] == ticker][['date', modelo_sent]].dropna()\n",
        "        if df_sent.empty:\n",
        "            print(f\"⚠️ Sem dados para {modelo_sent}. Pulando.\")\n",
        "            continue\n",
        "\n",
        "    df_sent['date'] = pd.to_datetime(df_sent['date']).dt.date\n",
        "    sentimento_dict = df_sent.set_index('date')[modelo_sent].to_dict()\n",
        "\n",
        "    capital = capital_inicial\n",
        "    posicao = 0\n",
        "    decisoes = []\n",
        "    operacoes = 0\n",
        "    acertos = 0\n",
        "    taxas_B3 = 0.0\n",
        "    IR_pago = 0.0\n",
        "    historico_valor = []\n",
        "\n",
        "    vendas_mes = defaultdict(float)\n",
        "    lucro_mes = defaultdict(float)\n",
        "    prejuizo_acumulado = 0.0\n",
        "    preco_medio = 0.0\n",
        "\n",
        "    for _, row in df_price.iterrows():\n",
        "        data = pd.to_datetime(row['date']).date()\n",
        "        preco = row['close']\n",
        "        sentimento = sentimento_dict.get(data, 0.0)\n",
        "        acao = \"MANTER\"\n",
        "        quantidade = 0\n",
        "        taxa = 0.0\n",
        "\n",
        "        # --- COMPRA ---\n",
        "        if sentimento > 0:\n",
        "            valor_para_compra = capital * sentimento\n",
        "            quantidade = int(valor_para_compra / preco)\n",
        "            if quantidade > 0:\n",
        "                valor_op = quantidade * preco\n",
        "                taxa = valor_op * taxa_b3\n",
        "                custo_total = valor_op + taxa\n",
        "                if capital >= custo_total:\n",
        "                    preco_medio = ((preco_medio * posicao) + valor_op) / (posicao + quantidade) if posicao > 0 else preco\n",
        "                    capital -= custo_total\n",
        "                    posicao += quantidade\n",
        "                    taxas_B3 += taxa\n",
        "                    operacoes += 1\n",
        "                    acao = f\"COMPRA {quantidade}\"\n",
        "\n",
        "        # --- VENDA ---\n",
        "        elif sentimento < 0 and posicao > 0:\n",
        "            quantidade = int(posicao * abs(sentimento))\n",
        "            if quantidade > 0:\n",
        "                valor_op = quantidade * preco\n",
        "                taxa = valor_op * taxa_b3\n",
        "                receita_liquida = valor_op - taxa\n",
        "                capital += receita_liquida\n",
        "                posicao -= quantidade\n",
        "                taxas_B3 += taxa\n",
        "                operacoes += 1\n",
        "                acao = f\"VENDA {quantidade}\"\n",
        "\n",
        "                lucro_real = (preco - preco_medio) * quantidade\n",
        "                mes = data.strftime('%Y-%m')\n",
        "                vendas_mes[mes] += valor_op\n",
        "\n",
        "                if lucro_real > 0:\n",
        "                    acertos += 1\n",
        "                    lucro_mes[mes] += lucro_real\n",
        "                else:\n",
        "                    prejuizo_acumulado += abs(lucro_real)\n",
        "\n",
        "        valor_total = capital + posicao * preco\n",
        "        historico_valor.append(valor_total)\n",
        "\n",
        "        decisoes.append({\n",
        "            'date': data,\n",
        "            'modelo': modelo_sent,\n",
        "            'sentimento': sentimento,\n",
        "            'close': preco,\n",
        "            'acao': acao,\n",
        "            'quantidade': quantidade,\n",
        "            'posicao': posicao,\n",
        "            'capital_em_dinheiro': capital,\n",
        "            'capital_total_estimado': valor_total,\n",
        "            'taxas_B3': taxas_B3\n",
        "        })\n",
        "\n",
        "    # === CÁLCULO DO IR ===\n",
        "    for mes in lucro_mes:\n",
        "        lucro = lucro_mes[mes] - prejuizo_acumulado\n",
        "        if lucro <= 0:\n",
        "            prejuizo_acumulado = abs(lucro)\n",
        "            continue\n",
        "        if vendas_mes[mes] > 20_000:\n",
        "            IR_pago += lucro * 0.15\n",
        "            prejuizo_acumulado = 0.0\n",
        "\n",
        "    # === MÉTRICAS ===\n",
        "    lucro_bruto = valor_total - capital_inicial\n",
        "    lucro_liquido = lucro_bruto - IR_pago\n",
        "    lucro_pct = (lucro_liquido / capital_inicial) * 100\n",
        "    retorno_diario = pd.Series(historico_valor).pct_change().dropna()\n",
        "    sharpe = retorno_diario.mean() / retorno_diario.std() * np.sqrt(252) if len(retorno_diario) > 1 else 0\n",
        "    max_drawdown = (pd.Series(historico_valor).cummax() - pd.Series(historico_valor)).max()\n",
        "    taxa_acerto = acertos / operacoes * 100 if operacoes > 0 else 0\n",
        "\n",
        "    curvas[modelo_sent] = pd.DataFrame(decisoes)\n",
        "\n",
        "    resultados.append({\n",
        "        'modelo': modelo_sent,\n",
        "        'valor_final': valor_total,\n",
        "        'lucro_bruto': lucro_bruto,\n",
        "        'IR_pago': IR_pago,\n",
        "        'lucro_liquido': lucro_liquido,\n",
        "        'lucro_pct': lucro_pct,\n",
        "        'n_operacoes': operacoes,\n",
        "        'taxa_acerto_pct': taxa_acerto,\n",
        "        'sharpe_ratio': sharpe,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'taxas_B3': taxas_B3\n",
        "    })\n",
        "\n",
        "# === RESUMO ===\n",
        "df_resultados = pd.DataFrame(resultados).sort_values(by='lucro_pct', ascending=False)\n",
        "print(\"\\n📊 COMPARATIVO COM MODELO ALEATÓRIO (Random Benchmark):\")\n",
        "display(df_resultados)\n",
        "\n",
        "# === PLOTAGEM ===\n",
        "plt.figure(figsize=(14, 7))\n",
        "for modelo, df in curvas.items():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    lucro = df_resultados[df_resultados['modelo'] == modelo]['lucro_pct'].values[0]\n",
        "    plt.plot(df['date'], df['capital_total_estimado'], label=f\"{modelo} ({lucro:.2f}%)\")\n",
        "\n",
        "plt.title(f\"Evolução da Carteira – Comparação com Benchmark Aleatório ({ticker})\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Data\")\n",
        "plt.ylabel(\"Capital Total (R$)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.xticks(rotation=45)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.tight_layout()\n",
        "plt.legend(title='Modelo (Lucro Líquido %)', fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dOAxzMDCNxZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTAÇÕES ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# === PARÂMETROS ===\n",
        "ticker = 'ITUB4.SA'\n",
        "modelos_sentimento = [\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment',\n",
        "    'sentiment_random'  # Benchmark aleatório corrigido\n",
        "]\n",
        "capital_inicial = 100_000\n",
        "taxa_b3 = 0.0003\n",
        "\n",
        "# === PERÍODO ===\n",
        "datas_validas = df_diario[df_diario['ticker'] == ticker]['date'].dropna()\n",
        "inicio = pd.to_datetime(datas_validas.min()).strftime('%Y-%m-%d')\n",
        "fim = pd.to_datetime(datas_validas.max()).strftime('%Y-%m-%d')\n",
        "print(f\"📅 Período: {inicio} a {fim}\")\n",
        "\n",
        "# === PREÇOS ===\n",
        "df_price = yf.download(ticker, start=inicio, end=fim, progress=False)\n",
        "if isinstance(df_price.columns, pd.MultiIndex):\n",
        "    df_price.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df_price.columns]\n",
        "df_price = df_price.reset_index()\n",
        "df_price['date'] = pd.to_datetime(df_price['Date']).dt.date\n",
        "df_price['close'] = df_price[f'Close_{ticker}'] if f'Close_{ticker}' in df_price.columns else df_price['Close']\n",
        "\n",
        "# === DIRETÓRIO ===\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# === RESULTADOS ===\n",
        "resultados = []\n",
        "curvas = {}\n",
        "\n",
        "for modelo_sent in modelos_sentimento:\n",
        "    print(f\"\\n▶️ Simulando modelo: {modelo_sent}\")\n",
        "\n",
        "    if modelo_sent == 'sentiment_random':\n",
        "        # Geração de sentimento aleatório com variabilidade autêntica\n",
        "        np.random.seed(None)  # garante aleatoriedade diferente em cada execução\n",
        "        ruido = np.random.normal(loc=0, scale=0.6, size=len(df_price))\n",
        "        ruido = np.clip(ruido, -1, 1)\n",
        "        df_sent = pd.DataFrame({\n",
        "            'date': df_price['date'],\n",
        "            'sentiment_random': ruido\n",
        "        })\n",
        "    else:\n",
        "        df_sent = df_diario[df_diario['ticker'] == ticker][['date', modelo_sent]].dropna()\n",
        "        if df_sent.empty:\n",
        "            print(f\"⚠️ Sem dados para {modelo_sent}. Pulando.\")\n",
        "            continue\n",
        "\n",
        "    df_sent['date'] = pd.to_datetime(df_sent['date']).dt.date\n",
        "    sentimento_dict = df_sent.set_index('date')[modelo_sent].to_dict()\n",
        "\n",
        "    capital = capital_inicial\n",
        "    posicao = 0\n",
        "    preco_medio = 0.0\n",
        "    decisoes = []\n",
        "    operacoes = 0\n",
        "    acertos = 0\n",
        "    taxas_B3 = 0.0\n",
        "    IR_pago = 0.0\n",
        "    historico_valor = []\n",
        "\n",
        "    vendas_mes = defaultdict(float)\n",
        "    lucro_mes = defaultdict(float)\n",
        "    prejuizo_acumulado = 0.0\n",
        "\n",
        "    for _, row in df_price.iterrows():\n",
        "        data = pd.to_datetime(row['date']).date()\n",
        "        preco = row['close']\n",
        "        sentimento = sentimento_dict.get(data, 0.0)\n",
        "\n",
        "        acao = \"MANTER\"\n",
        "        quantidade = 0\n",
        "        taxa = 0.0\n",
        "\n",
        "        if sentimento > 0:\n",
        "            valor_para_compra = capital * sentimento\n",
        "            quantidade = int(valor_para_compra / preco)\n",
        "            if quantidade > 0:\n",
        "                valor_op = quantidade * preco\n",
        "                taxa = valor_op * taxa_b3\n",
        "                custo_total = valor_op + taxa\n",
        "                if capital >= custo_total:\n",
        "                    preco_medio = ((preco_medio * posicao) + valor_op) / (posicao + quantidade) if posicao > 0 else preco\n",
        "                    capital -= custo_total\n",
        "                    posicao += quantidade\n",
        "                    taxas_B3 += taxa\n",
        "                    operacoes += 1\n",
        "                    acao = f\"COMPRA {quantidade}\"\n",
        "\n",
        "        elif sentimento < 0 and posicao > 0:\n",
        "            quantidade = int(posicao * abs(sentimento))\n",
        "            if quantidade > 0:\n",
        "                valor_op = quantidade * preco\n",
        "                taxa = valor_op * taxa_b3\n",
        "                receita_liquida = valor_op - taxa\n",
        "                capital += receita_liquida\n",
        "                posicao -= quantidade\n",
        "                taxas_B3 += taxa\n",
        "                operacoes += 1\n",
        "                acao = f\"VENDA {quantidade}\"\n",
        "\n",
        "                lucro_real = (preco - preco_medio) * quantidade\n",
        "                mes = data.strftime('%Y-%m')\n",
        "                vendas_mes[mes] += valor_op\n",
        "\n",
        "                if lucro_real > 0:\n",
        "                    acertos += 1\n",
        "                    lucro_mes[mes] += lucro_real\n",
        "                else:\n",
        "                    prejuizo_acumulado += abs(lucro_real)\n",
        "\n",
        "        valor_total = capital + posicao * preco\n",
        "        historico_valor.append(valor_total)\n",
        "\n",
        "        decisoes.append({\n",
        "            'date': data,\n",
        "            'modelo': modelo_sent,\n",
        "            'sentimento': sentimento,\n",
        "            'close': preco,\n",
        "            'acao': acao,\n",
        "            'quantidade': quantidade,\n",
        "            'posicao': posicao,\n",
        "            'capital_em_dinheiro': capital,\n",
        "            'capital_total_estimado': valor_total,\n",
        "            'taxas_B3': taxas_B3\n",
        "        })\n",
        "\n",
        "    # === CÁLCULO DO IR ===\n",
        "    for mes in lucro_mes:\n",
        "        lucro = lucro_mes[mes] - prejuizo_acumulado\n",
        "        if lucro <= 0:\n",
        "            prejuizo_acumulado = abs(lucro)\n",
        "            continue\n",
        "        if vendas_mes[mes] > 20_000:\n",
        "            IR_pago += lucro * 0.15\n",
        "            prejuizo_acumulado = 0.0\n",
        "\n",
        "    lucro_bruto = valor_total - capital_inicial\n",
        "    lucro_liquido = lucro_bruto - IR_pago\n",
        "    lucro_pct = (lucro_liquido / capital_inicial) * 100\n",
        "    retorno_diario = pd.Series(historico_valor).pct_change().dropna()\n",
        "    sharpe = retorno_diario.mean() / retorno_diario.std() * np.sqrt(252) if len(retorno_diario) > 1 else 0\n",
        "    max_drawdown = (pd.Series(historico_valor).cummax() - pd.Series(historico_valor)).max()\n",
        "    taxa_acerto = acertos / operacoes * 100 if operacoes > 0 else 0\n",
        "\n",
        "    curvas[modelo_sent] = pd.DataFrame(decisoes)\n",
        "\n",
        "    resultados.append({\n",
        "        'modelo': modelo_sent,\n",
        "        'valor_final': valor_total,\n",
        "        'lucro_bruto': lucro_bruto,\n",
        "        'IR_pago': IR_pago,\n",
        "        'lucro_liquido': lucro_liquido,\n",
        "        'lucro_pct': lucro_pct,\n",
        "        'n_operacoes': operacoes,\n",
        "        'taxa_acerto_pct': taxa_acerto,\n",
        "        'sharpe_ratio': sharpe,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'taxas_B3': taxas_B3\n",
        "    })\n",
        "\n",
        "# === RESUMO FINAL ===\n",
        "df_resultados = pd.DataFrame(resultados).sort_values(by='lucro_pct', ascending=False)\n",
        "print(\"\\n📊 COMPARATIVO COM BENCHMARK ALEATÓRIO:\")\n",
        "display(df_resultados)\n",
        "\n",
        "# === PLOTAGEM ===\n",
        "plt.figure(figsize=(14, 7))\n",
        "for modelo, df in curvas.items():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    lucro = df_resultados[df_resultados['modelo'] == modelo]['lucro_pct'].values[0]\n",
        "    plt.plot(df['date'], df['capital_total_estimado'], label=f\"{modelo} ({lucro:.2f}%)\")\n",
        "\n",
        "plt.title(f\"Evolução da Carteira – Benchmark Aleatório ({ticker})\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Data\")\n",
        "plt.ylabel(\"Capital Total (R$)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.xticks(rotation=45)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.tight_layout()\n",
        "plt.legend(title='Modelo (Lucro Líquido %)', fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0RruIG8EUBRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGdZLoMpaVHc"
      },
      "source": [
        "<a id='5.2'></a>\n",
        "## 5.2. Results for Individual Stocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVXPJgl5aVHc"
      },
      "source": [
        "First running the strategy for google"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jJxVwJolQq1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTAÇÕES ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# === PARÂMETROS ===\n",
        "ticker = 'VALE3.SA'\n",
        "modelos_sentimento = [\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment',\n",
        "    'sentiment_random'\n",
        "]\n",
        "capital_inicial = 100_000\n",
        "taxa_b3 = 0.0003  # 0,03%\n",
        "loss_aversion_ratio = 2.25  # fator de aversão à perda\n",
        "\n",
        "# === PERÍODO ===\n",
        "datas_validas = df_diario[df_diario['ticker'] == ticker]['date'].dropna()\n",
        "inicio = pd.to_datetime(datas_validas.min()).strftime('%Y-%m-%d')\n",
        "fim = pd.to_datetime(datas_validas.max()).strftime('%Y-%m-%d')\n",
        "print(f\"📅 Período: {inicio} a {fim}\")\n",
        "\n",
        "# === PREÇOS ===\n",
        "df_price = yf.download(ticker, start=inicio, end=fim, progress=False)\n",
        "if isinstance(df_price.columns, pd.MultiIndex):\n",
        "    df_price.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df_price.columns]\n",
        "df_price = df_price.reset_index()\n",
        "df_price['date'] = pd.to_datetime(df_price['Date']).dt.date\n",
        "col_close = f'Close_{ticker}' if f'Close_{ticker}' in df_price.columns else 'Close'\n",
        "df_price['close'] = df_price[col_close]\n",
        "\n",
        "# === DIRETÓRIO ===\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# === RESULTADOS GERAIS ===\n",
        "resultados = []\n",
        "curvas = {}\n",
        "\n",
        "for modelo_sent in modelos_sentimento:\n",
        "    print(f\"\\n▶️ Modelo proporcional: {modelo_sent}\")\n",
        "\n",
        "    if modelo_sent == 'sentiment_random':\n",
        "        df_sent = pd.DataFrame({\n",
        "            'date': df_price['date'],\n",
        "            'sentiment_random': np.random.uniform(-1, 1, size=len(df_price))\n",
        "        })\n",
        "    else:\n",
        "        df_sent = df_diario[df_diario['ticker'] == ticker][['date', modelo_sent]].dropna()\n",
        "        if df_sent.empty:\n",
        "            print(f\"⚠️ Sem dados para {modelo_sent}. Pulando.\")\n",
        "            continue\n",
        "\n",
        "    df_sent['date'] = pd.to_datetime(df_sent['date']).dt.date\n",
        "    sentimento_dict = df_sent.set_index('date')[modelo_sent].to_dict()\n",
        "\n",
        "    capital = capital_inicial\n",
        "    posicao = 0\n",
        "    preco_medio = 0.0\n",
        "    decisoes = []\n",
        "    operacoes = 0\n",
        "    acertos = 0\n",
        "    taxas_B3 = 0.0\n",
        "    IR_pago = 0.0\n",
        "    historico_valor = []\n",
        "\n",
        "    vendas_mes = defaultdict(float)\n",
        "    lucro_mes = defaultdict(float)\n",
        "    prejuizo_acumulado = 0.0\n",
        "\n",
        "    for _, row in df_price.iterrows():\n",
        "        data = pd.to_datetime(row['date']).date()\n",
        "        preco = row['close']\n",
        "        sentimento = sentimento_dict.get(data, 0.0)\n",
        "\n",
        "        # Ajuste com aversão à perda\n",
        "        peso = sentimento if sentimento >= 0 else sentimento * loss_aversion_ratio\n",
        "\n",
        "        acao = \"MANTER\"\n",
        "        quantidade = 0\n",
        "        taxa = 0.0\n",
        "\n",
        "        # --- COMPRA proporcional ---\n",
        "        if peso > 0:\n",
        "            valor_para_compra = capital * peso\n",
        "            quantidade = int(valor_para_compra / preco)\n",
        "            if quantidade > 0:\n",
        "                valor_op = quantidade * preco\n",
        "                taxa = valor_op * taxa_b3\n",
        "                custo_total = valor_op + taxa\n",
        "                if capital >= custo_total:\n",
        "                    preco_medio = ((preco_medio * posicao) + valor_op) / (posicao + quantidade) if posicao > 0 else preco\n",
        "                    capital -= custo_total\n",
        "                    posicao += quantidade\n",
        "                    taxas_B3 += taxa\n",
        "                    operacoes += 1\n",
        "                    acao = f\"COMPRA {quantidade}\"\n",
        "\n",
        "        # --- VENDA proporcional ---\n",
        "        elif peso < 0 and posicao > 0:\n",
        "            quantidade = int(posicao * abs(peso))\n",
        "            if quantidade > 0:\n",
        "                valor_op = quantidade * preco\n",
        "                taxa = valor_op * taxa_b3\n",
        "                receita_liquida = valor_op - taxa\n",
        "                capital += receita_liquida\n",
        "                posicao -= quantidade\n",
        "                taxas_B3 += taxa\n",
        "                operacoes += 1\n",
        "                acao = f\"VENDA {quantidade}\"\n",
        "\n",
        "                lucro_real = (preco - preco_medio) * quantidade\n",
        "                mes = data.strftime('%Y-%m')\n",
        "                vendas_mes[mes] += valor_op\n",
        "\n",
        "                if lucro_real > 0:\n",
        "                    acertos += 1\n",
        "                    lucro_mes[mes] += lucro_real\n",
        "                else:\n",
        "                    prejuizo_acumulado += abs(lucro_real)\n",
        "\n",
        "        valor_total = capital + posicao * preco\n",
        "        historico_valor.append(valor_total)\n",
        "\n",
        "        decisoes.append({\n",
        "            'date': data,\n",
        "            'modelo': modelo_sent,\n",
        "            'sentimento': sentimento,\n",
        "            'close': preco,\n",
        "            'acao': acao,\n",
        "            'quantidade': quantidade,\n",
        "            'posicao': posicao,\n",
        "            'capital_em_dinheiro': capital,\n",
        "            'capital_total_estimado': valor_total,\n",
        "            'taxas_B3': taxas_B3\n",
        "        })\n",
        "\n",
        "    # === CÁLCULO DE IR ===\n",
        "    for mes in lucro_mes:\n",
        "        lucro = lucro_mes[mes] - prejuizo_acumulado\n",
        "        if lucro <= 0:\n",
        "            prejuizo_acumulado = abs(lucro)\n",
        "            continue\n",
        "        if vendas_mes[mes] > 20_000:\n",
        "            IR_pago += lucro * 0.15\n",
        "            prejuizo_acumulado = 0.0\n",
        "\n",
        "    # === MÉTRICAS FINAIS ===\n",
        "    lucro_bruto = valor_total - capital_inicial\n",
        "    lucro_liquido = lucro_bruto - IR_pago\n",
        "    lucro_pct = (lucro_liquido / capital_inicial) * 100\n",
        "    retorno_diario = pd.Series(historico_valor).pct_change().dropna()\n",
        "    sharpe = retorno_diario.mean() / retorno_diario.std() * np.sqrt(252) if len(retorno_diario) > 1 else 0\n",
        "    max_drawdown = (pd.Series(historico_valor).cummax() - pd.Series(historico_valor)).max()\n",
        "    taxa_acerto = acertos / operacoes * 100 if operacoes > 0 else 0\n",
        "\n",
        "    curvas[modelo_sent] = pd.DataFrame(decisoes)\n",
        "\n",
        "    resultados.append({\n",
        "        'modelo': modelo_sent,\n",
        "        'valor_final': valor_total,\n",
        "        'lucro_bruto': lucro_bruto,\n",
        "        'IR_pago': IR_pago,\n",
        "        'lucro_liquido': lucro_liquido,\n",
        "        'lucro_pct': lucro_pct,\n",
        "        'n_operacoes': operacoes,\n",
        "        'taxa_acerto_pct': taxa_acerto,\n",
        "        'sharpe_ratio': sharpe,\n",
        "        'max_drawdown': max_drawdown,\n",
        "        'taxas_B3': taxas_B3\n",
        "    })\n",
        "\n",
        "# === RESUMO FINAL ===\n",
        "df_resultados = pd.DataFrame(resultados).sort_values(by='lucro_pct', ascending=False)\n",
        "print(\"\\n📊 RESUMO FINAL COM APLICAÇÃO DE AVERSÃO À PERDA:\")\n",
        "display(df_resultados)\n",
        "\n",
        "# === PLOTAGEM ===\n",
        "plt.figure(figsize=(14, 7))\n",
        "for modelo, df in curvas.items():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    lucro = df_resultados[df_resultados['modelo'] == modelo]['lucro_pct'].values[0]\n",
        "    plt.plot(df['date'], df['capital_total_estimado'], label=f\"{modelo} ({lucro:.2f}%)\")\n",
        "\n",
        "plt.title(f\"Evolução da Carteira – Aversão à Perda e Benchmark Aleatório ({ticker})\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Data\")\n",
        "plt.ylabel(\"Capital Total (R$)\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.xticks(rotation=45)\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.tight_layout()\n",
        "plt.legend(title='Modelo (Lucro Líquido %)', fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AnGBy7zvQqDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from collections import defaultdict\n",
        "\n",
        "# === PARÂMETROS ===\n",
        "tickers = ['VALE3.SA', 'PETR4.SA', 'ITUB4.SA', 'BBAS3.SA']\n",
        "modelos_sent = [\n",
        "    'sentiment_lex',\n",
        "    'sentiment_LSTM',\n",
        "    'sentiment_FinBERT_PT_BR',\n",
        "    'sentiment_FinBERT_Turing',\n",
        "    'sentiment_Multilingual_Sentiment',\n",
        "    'sentiment_random'\n",
        "]\n",
        "capital_inicial = 100_000\n",
        "taxa_b3 = 0.0003\n",
        "start = '2025-01-01'\n",
        "end   = '2025-05-04'\n",
        "\n",
        "# Estruturas para armazenar curvas\n",
        "curvas_hold  = {}\n",
        "curvas_trade = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    # 1. Obtenção de preços\n",
        "    df_price = yf.download(ticker, start=start, end=end, progress=False).reset_index()\n",
        "    df_price['date']  = pd.to_datetime(df_price['Date']).dt.date\n",
        "    # normalização de coluna de fechamento\n",
        "    close_col = f'Close_{ticker}'\n",
        "    df_price['close'] = (df_price[close_col] if close_col in df_price else df_price['Close'])\n",
        "\n",
        "    # 2. Cálculo buy-and-hold\n",
        "    preco0 = df_price.loc[0, 'close']\n",
        "    n_acoes = int(capital_inicial / preco0)\n",
        "    sobra    = capital_inicial - n_acoes * preco0\n",
        "    df_hold = df_price[['date']].copy()\n",
        "    df_hold['capital_hold'] = n_acoes * df_price['close'] + sobra\n",
        "    curvas_hold[ticker] = df_hold\n",
        "\n",
        "    # 3. Simulação de trading\n",
        "    # (pressupõe existência de df_diario com sentimentos para todos os tickers)\n",
        "    df_sent_all = df_diario[df_diario['ticker']==ticker]\n",
        "    capital = capital_inicial\n",
        "    posicao = 0\n",
        "    preco_medio = 0.0\n",
        "    historico = []\n",
        "    # geração de sentimento aleatório\n",
        "    for modelo in modelos_sent:\n",
        "        if modelo=='sentiment_random':\n",
        "            ruido = np.random.normal(0,0.6, size=len(df_price))\n",
        "            ruido = np.clip(ruido, -1, 1)\n",
        "            df_sent = pd.DataFrame({'date':df_price['date'],'sentiment_random':ruido})\n",
        "        else:\n",
        "            df_tmp = df_sent_all[['date',modelo]].dropna()\n",
        "            df_sent = df_tmp.copy()\n",
        "        sentimento_dict = df_sent.set_index(pd.to_datetime(df_sent['date']).dt.date)[modelo].to_dict()\n",
        "        capital = capital_inicial; posicao=0; preco_medio=0.0; taxas_B3=0.0\n",
        "        historico = []\n",
        "        for row in df_price.itertuples():\n",
        "            d = row.date; p = row.close\n",
        "            s = sentimento_dict.get(d, 0.0)\n",
        "            # compra\n",
        "            if s>0:\n",
        "                v = capital * s\n",
        "                q = int(v/p)\n",
        "                if q>0:\n",
        "                    op = q*p; tx = op*taxa_b3\n",
        "                    if capital>=op+tx:\n",
        "                        preco_medio = ((preco_medio*posicao)+op)/(posicao+q) if posicao>0 else p\n",
        "                        capital -= op+tx\n",
        "                        posicao += q\n",
        "                        taxas_B3 += tx\n",
        "            # venda\n",
        "            elif s<0 and posicao>0:\n",
        "                q = int(posicao * abs(s))\n",
        "                if q>0:\n",
        "                    op = q*p; tx = op*taxa_b3\n",
        "                    capital += op-tx\n",
        "                    posicao -= q\n",
        "                    taxas_B3 += tx\n",
        "            total = capital + posicao*p\n",
        "            historico.append({'date':d, 'capital': total})\n",
        "        curvas_trade.setdefault(ticker, pd.DataFrame(historico))\n",
        "\n",
        "# 4. Plotagem comparativa em subplots\n",
        "n = len(tickers)\n",
        "fig, axes = plt.subplots(n, 1, figsize=(12, 4*n), sharex=True)\n",
        "for ax, ticker in zip(axes, tickers):\n",
        "    df_h = curvas_hold[ticker]\n",
        "    df_t = curvas_trade[ticker]\n",
        "    df_h['date'] = pd.to_datetime(df_h['date'])\n",
        "    df_t['date'] = pd.to_datetime(df_t['date'])\n",
        "    ax.plot(df_h['date'], df_h['capital_hold'],\n",
        "            '--', color='gray', label='Buy-and-Hold')\n",
        "    ax.plot(df_t['date'], df_t['capital'],\n",
        "            linewidth=1.5, label='Trading')\n",
        "    ax.set_title(f'{ticker}', fontweight='bold')\n",
        "    ax.grid(linestyle=':', alpha=0.6)\n",
        "    ax.legend()\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    for label in ax.get_xticklabels():\n",
        "        label.set_rotation(45)\n",
        "plt.xlabel('Data')\n",
        "plt.ylabel('Capital Total (R$)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VIf5kzQjbQws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yI-yP6DaZrsM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "_change_revision": 206,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}